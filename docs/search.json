[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "Outline: Working with Data in PyTorch for Deep Learning",
    "crumbs": [
      "Home",
      "Advanced",
      "Data"
    ]
  },
  {
    "objectID": "data.html#analyze-data",
    "href": "data.html#analyze-data",
    "title": "Data",
    "section": "Analyze Data",
    "text": "Analyze Data\nUnderstanding the structure and content of your data is paramount to good data-driven models\n\nimport pandas as pd\n\ndf = pd.read_pickle(\"_dlcourse/code/RegressionData.pkl\") #This could be any file format\n\npd.set_option('display.colheader_justify', 'center') #pretty print option\npd.set_option('display.precision',3) #pretty print option\n\nprint(f\"Total number of samples: {len(df)}\")\nprint(\"Top 5 rows:\")\nprint(df.head())\n\nTotal number of samples: 100\nTop 5 rows:\n     A      B      C      D      E      F    Target_0\n0  0.730  5.540  2.244 -1.601 -1.691  0.625   30.720 \n1  0.228 -8.955  2.642  7.802 -0.855 -0.304  -54.216 \n2 -0.835 -1.549  2.271  0.314  0.071 -5.181  -27.621 \n3  0.628 -0.536 -0.007  5.097  1.917 -2.232  -23.083 \n4  0.474  7.877 -0.879  2.787  1.869 -5.179    3.191 \n\n\nIn this example we have 6 inputs (A thru F) and 1 output (Target_0) of all numeric data. While not required, it is useful to create a torch dataset object to manage it. My prefered data flow is shown below, but it is up to you how to structure and handle your own projects.\n\n\n\n\n\nflowchart LR\n  A((Data File)) --&gt; B(Dataset)\n  B --&gt; C((Prepared &lt;br/&gt;Tensors))\n  C --&gt; D(Dataloader)\n  D --&gt; E((Batches))\n  E--&gt; F(Model)",
    "crumbs": [
      "Home",
      "Advanced",
      "Data"
    ]
  },
  {
    "objectID": "data.html#create-a-dataset",
    "href": "data.html#create-a-dataset",
    "title": "Data",
    "section": "Create a Dataset",
    "text": "Create a Dataset\nWhenever you want to create a custom pytorch Dataset, there are three requirements: Inhereit the Dataset class, create a len function, and create a getitem function.\n\nfrom torch.utils.data import Dataset\n\nclass SomeDataset(Dataset):\n    def __init__(self,...):\n        super().__init__() #calls the init function of super class Dataset\n        ...\n    \n    def __len(self):\n      ...\n    \n    def __getitem__(self, index):\n      ...\n\nYou can include any other function and pass any number of initial class inputs, but these three requirements must be met.\nFor this example we are going to use the RegressionDataset class.\n\nclass RegressionDataset(Dataset):\n    def __init__(self,\n                 data_file:str,\n                 normalize:bool=False,\n                 ):\n        super().__init__()\n\nHere we are creating the class RegressionDataset which is a subclass of torch.utils.data.Dataset. It will take the argument data_file as the path to the regression data, and optionally normalize which will normalize the data from 0 to 1\n\ndf_data:pd.DataFrame = pd.read_pickle(data_file) #opens base PKL file\ntarget_labels = [c for c in df_data.columns if c.startswith('Target')] #Finds all target columns\ndf_inputs = df_data.drop(target_labels,axis=1) # Creates DF of only inputs\ndf_targets = df_data[target_labels] # Creates DF of only targets\n\nHere we have loaded all the data and split up the inputs and the targets.\n\nself._tensor_inputs = torch.tensor(df_inputs.values).float()\nself._tensor_targets = torch.tensor(df_targets.values).view(-1,len(target_labels)).float()\n\nHere is an easy pitfall in data management. This chunk converts the code into tensors and shapes them into the appropriate size.\ndf.values → Converts DataFrame in a numpy array .float() → Converts the Double(float64) tensor into Float(float32) .view(-1,len(target_labels)) → Converts the 1D target tensor into a 2D tensor\nOne thing we can check is the documentation of the layer we are going to be using, Linear. This informs the structure of the data that needs to be fed into the network. We see that the layer takes (*,Hin​), so all of the “action” will be taking place on the last dimension of your inputs.\n\ndef __len__(self):\n    return len(self._tensor_targets)\n\ndef __getitem__(self, index):\n    inputs = self._tensor_inputs[index]\n    targets = self._tensor_targets[index]\n    return inputs,targets\n\nSkipping ahead, these are the other two required parts of a Dataset. the len function needs to return the total number of samples (that will be going across axis 0). and the getitem function returns the data that your network is utilizing based on some index\n\n## Data Representation in PyTorch\nif normalize:\n    self.normalize(self._tensor_inputs)\n    self.normalize(self._tensor_targets)\n\ndef normalize(self,tensor:torch.Tensor) -&gt; None:\n      tensor[:] = (tensor-tensor.amin(0))/(tensor.amax(0)-tensor.amin(0))\n\nThis last chunk of code does a basic 0 to 1 normalization of the data (i.e. it does not fit it to a gaussian). I prefer to have my dataset class handle transformations, normalizations, and augmentations.",
    "crumbs": [
      "Home",
      "Advanced",
      "Data"
    ]
  },
  {
    "objectID": "data.html#create-a-dataloader",
    "href": "data.html#create-a-dataloader",
    "title": "Data",
    "section": "Create a Dataloader",
    "text": "Create a Dataloader\nI prefer to make a DataLoader class to handle the management of my model data. This is not required, but it tends to clean up my code and make life easier in the long run\n\n## Data Representation in PyTorch\n\nclass MLPDataLoader():\n    def __init__(\n            self,\n            dataset,\n            train_split:float = .8,\n            batch_size:int = 10,\n\n        ):\n        indices = list(range(len(dataset)))\n        random.shuffle(indices)\n        split = int(len(dataset)*train_split)\n        self._batch_size = batch_size\n        self._train_idx = indices[:split]\n        self._val_idx = indices[split:]\n        self._trainset = Subset(dataset,self._train_idx)\n        self._valset = Subset(dataset,self._val_idx)\n\n    def train_dataloader(self):\n        return DataLoader(\n            self._trainset,\n            batch_size=self._batch_size,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self._valset,\n            batch_size=self._batch_size,\n            shuffle=False,\n        )",
    "crumbs": [
      "Home",
      "Advanced",
      "Data"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "This course focuses on supervised machine learning. Two areas of machine learning we do not cover involve self-supervised learning and reinforcement learning.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#setup",
    "href": "introduction.html#setup",
    "title": "Introduction",
    "section": "Setup",
    "text": "Setup\nWe use the following packages:\n\nnumpy: working with \\(n\\)-D arrays\ntorch: training of differentiable models\ntorchvision: tools for vision models\npandas: working with data\nmatplotlib: plotting\ntqdm: progress bars in the command line\nscikit-learn: source some classic datasets\n\nThe following command uses pip to install these python packages:\npip install torch torchvision numpy matplotlib tqdm pandas scikit-learn\n\nVirtual Environment\nVirtual environments allow you to install a custom set of packages with custom package versions for a project.\nInside the dlcourse directory, run\npython -m venv code-venv\nto create a blank virtual environment.\nActivate it, and then install packages. Upon activation, the command line prompt changes to start with the name of the virtual environment. Here, it would be (code-venv):\nsource code-venv/bin/activate\npip install torch torchvision numpy matplotlib tqdm pandas scikit-learn\nWhen you are done, you can run deactivate to return to the default global environment.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#numpy",
    "href": "introduction.html#numpy",
    "title": "Introduction",
    "section": "NumPy",
    "text": "NumPy\nA nice overview of using numpy is located online here.\nTwo KEY issues to watch out for:\n\nStandard matrix multiplication in numpy uses the @ symbol, not the * symbol. Users coming from Matlab or Julia often trip over this issue a few times until they internalize the new symbol.\n2D Arrays are stored in row-major form.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "ResNet.html",
    "href": "ResNet.html",
    "title": "Residual Neural Network",
    "section": "",
    "text": "Outline: Creating a residual neural network",
    "crumbs": [
      "Home",
      "Advanced",
      "Residual Neural Network"
    ]
  },
  {
    "objectID": "ResNet.html#skip-connections",
    "href": "ResNet.html#skip-connections",
    "title": "Residual Neural Network",
    "section": "Skip Connections",
    "text": "Skip Connections\nTheoretically, you cannot have too many layers in a network. For example, if a perfect network has 3 layers, then any excessive layer should just not do anything. So a deeper network should always have atleast the same performance as a shallower one…\nThis is not the case. Primarily it is theorized that layers do not like to become the identity function, so they negatively impact performance. To address this, we can effectivly add an identity function to our network that bypasses the network layers known as a skip connection.\n\n\n\n\n\n---\ntitle: RES\n---\nflowchart LR\n  D(Input) --&gt; E[Layer]\n  E --&gt; F(($$+$$))\n  D --&gt; F\n  F --&gt; G(Output)",
    "crumbs": [
      "Home",
      "Advanced",
      "Residual Neural Network"
    ]
  },
  {
    "objectID": "ResNet.html#residual-networks",
    "href": "ResNet.html#residual-networks",
    "title": "Residual Neural Network",
    "section": "Residual Networks",
    "text": "Residual Networks\nLet’s build a basic linear residual network, in essence it is just a mlp with a skip connection\n\nclass ResidualModel(nn.Module):\n    def __init__(self,\n                 in_neurons:int,\n                 hidden_neurons:int,\n                 out_neurons:int,\n                 dropout:float = 0.,\n                )-&gt;None:\n        \n        super().__init__()\n\n        layers = [\n                nn.Linear(in_neurons,hidden_neurons),\n                nn.Relu(),\n                nn.Dropout(dropout),\n                nn.Linear(hidden_neurons,out_neurons),\n                ]\n                \n        self.dense = nn.Sequential(*layers)\n        self.skip = nn.Linear(in_neurons,out_neurons)\n\n    def forward(self,x) -&gt; torch.Tensor:\n        x = self.dense(x) + self.skip(x)\n        return x\n\nThis is a completely viable network, but it isn’t really how these are used. The idea of a residual block is for a bunch of them to be commbined in sequence to form a “deep” network. To make this, we can just copy-paste our layers multiple times, or we can create a block.\n\nclass ResidualBlock(nn.Module):\n    def __init__(self,\n                 in_neurons:int,\n                 hidden_neurons:int,\n                 out_neurons:int,\n                 dropout:float = 0.,\n                 use_layer_norm:bool = False,\n                )-&gt;None:\n        \n        super().__init__()\n\n        layers = [\n                nn.Linear(in_neurons,hidden_neurons),\n                nn.Relu(),               \n                nn.Linear(hidden_neurons,out_neurons),\n                nn.Dropout(dropout),\n                ]\n                \n        self.dense = nn.Sequential(*layers)\n        self.skip = nn.Linear(in_neurons,out_neurons)\n        self.layer_norm = nn.LayerNorm(out_neurons) if use_layer_norm else None\n\n    def forward(self,x) -&gt; torch.Tensor:\n        x = self.dense(x) + self.skip(x)\n\n        if self.layer_norm is not None:\n                x = self.layer_norm(x)        \n        return x\n\nNotice that we moved the drop out to the end of block, and added the ability to apply a layer norm to the entire block\nWe can now generate a model using our residual blocks, basically the same way we made our original MLP\n\nclass ResNet(nn.Module):\n    def __init__(self,\n                 in_neurons:int,\n                 hidden_neurons:int,\n                 out_neurons:int,\n                 no_blocks: int,\n                 dropout:float = 0.,\n                 use_layer_norm:bool = False,\n                )-&gt;None:\n        \n        super().__init__()\n\n\n        layers = [ResidualBlock(in_neurons,hidden_neurons,hidden_neurons,dropout,use_layer_norm)]\n                \n        for i in range(no_blocks):\n                layers.append(ResidualBlock(hidden_neurons,hidden_neurons,hidden_neurons,dropout,use_layer_norm))\n\n        layers.append(ResidualBlock(hidden_neurons,hidden_neurons,out_neurons,dropout,use_layer_norm))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self,x) -&gt; torch.Tensor:\n        x = self.model(x)     \n        return x",
    "crumbs": [
      "Home",
      "Advanced",
      "Residual Neural Network"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning Course",
    "section": "",
    "text": "Course github: poonawalalab/dlcourse\n\nIntroduction\n\nNumPy\n\nSimple Models\nPyTorch\nMultilayer Perceptrons (MLPs)\nDatasets\nCNNs\n\nConvolutions"
  },
  {
    "objectID": "Neural_Networks.html",
    "href": "Neural_Networks.html",
    "title": "\nIntroduction\n",
    "section": "",
    "text": "Introduction\n\n\nConsider a model ŷ = f(x,θ)\n\n\nThe loss of the model is \\(L_{\\theta}(x,y) =\n\\frac{1}{2} \\lVert y -f(x,\\theta) \\rVert^2\\).\n\n\nThe partial derivative of L with respect to θ is then \\[\\nabla_{\\theta} L_{\\theta}(x,y) =\n-\\underbrace{\\underbrace{(y -f(x,\\theta))^T}_{\\text{co-vector}}\n\\underbrace{\\nabla_{\\theta}f(x,\\theta)}_{\\text{matrix}}\n}_{\\text{co-vector}}\\]\n\n\nNow, the first row of ∇θf(x,θ) corresponds to f1(x,θ), the second row to f2(x,θ), and so on. The first column of ∇θf(x,θ) corresponds to θ1, and so on again. Clearly, we’re assuming that θ is a vector, or a vectorized version of the parameters.\n\n\nA gradient descent approach leads to parameter updates of the form θ (a vector) as θk + 1 ← θk + α(∇θf(x,θk))T(y−f(x,θk)), where α is a parameter that controls the step size of the update, and hence called a learning rate.\n\n\nNote that gradient descent corresponds to θk + 1 ← θk + v where v is a vector that solves \\[\\begin{aligned}\n  \\min &amp; \\quad \\langle \\nabla_{\\theta} L_{\\theta}(x,y) , v\\rangle \\\\\n  \\text{subject to}&amp; \\quad \\lVert v \\rVert_2 \\leq \\alpha \\lVert\n\\nabla_{\\theta} L_{\\theta}(x,y) \\rVert_2\n\\end{aligned}\\]\n\n\nSimple Models\n\n\nLinear Models\n\n\nWe derive the gradient updates for fitting a linear model using quadratic loss function.\n\n\nMatrix Calculus\n\n\nLet our model be a linear model ŷ = f(x,θ) = Wx + b,  ⟹ δ = y − f(x,θ) We want to minimize the error \\(L =\n\\frac{1}{2}\\lVert \\delta\\rVert^2 =  \\sum_i \\frac{1}{2}\n\\delta_i^2\\). The gradient is simply \\[\\begin{aligned}\n\\frac{\\partial L}{\\partial W} &amp;= \\frac{\\partial L}{\\partial\n\\delta}\\frac{\\partial \\delta}{\\partial W} =   \\sum_i\n\\delta_i  \\frac{\\partial \\delta_i}{\\partial W}=   \\sum_i\n\\delta_i  \\frac{\\partial (-f_i(x,\\theta))}{\\partial W}\\\\\n&amp;=- \\sum_i \\delta_i \\frac{\\partial (e_i^T W x)}{\\partial W}\n\\end{aligned}\\]\n\n\nSince \\(\\frac{\\partial a^T X b}{\\partial X}\n= a b^T\\) (derivation below), we get \\[\\begin{aligned}\n\\frac{\\partial L}{\\partial W} &amp;=  - \\sum_i \\delta_i e_i x^T=  -\n\\left(  \\sum_i \\delta_i e_i \\right) x^T = \\underbrace{- \\delta x^T}_{n\n\\times n\\text{ matrix}}\n\\end{aligned}\\] The matrix update for W and vector update for b under gradient descent becomes Wk + 1 ← Wk + ηδk xT, bk + 1 ← bk + ηδk, where δk = y − f(x,θk), η is the learning rate parameter, and k refers to time, not an element index.\n\n\nDerivation.\n\n\nNote that f(X) = aTXb = ∑i∑jaixijbj. Clearly, \\(\\frac{\\partial f} {\\partial x_{ij}}\n= a_i b_j\\). Therefore, if we represent \\(\\frac{\\partial f} {\\partial X}\\) as a matrix, we get \\[\\begin{aligned}\n\\frac{\\partial } {\\partial X} (a^T X b)&amp;= a b^T  \n\\end{aligned}\\]\n\n\nVector Calculus\n\n\nThis section derives the same updates as in the previous one, using a vectorization of parameters, instead of matrix calculus. This section may be skipped.\n\n\nLet our model be a linear model f(x,θ) = Wx + b, with W = {wij} and b = {bi}, and θ being a vectorized version of W and b: \\[\\theta^T = \\begin{bmatrix} W^1 &amp; W^2\n&amp;  \\cdots &amp; W^n &amp; b^T \\end{bmatrix},\\] \\[= \\begin{bmatrix} w_{11} &amp; w_{12} &amp;\n\\cdots &amp; w_{n1}&amp;  \\cdots&amp;w_{nn} &amp; b_1 &amp; \\cdots &amp;\nb_n \\end{bmatrix}\\] where transposing θ makes it easier to associate each column of ∇θf(x,θ) with the right parameter.\n\n\nLet’s compute some partial derivatives. To start, notice that fi(x,θ) = wi1x1 + wi2x2 + … + winxn + bi. Therefore, \\[\\frac{\\partial f_i}{\\partial\nw_{jk}}  =\n\\begin{cases}\nx_k &amp; \\text{ if }i = j\\\\\n0 &amp; \\text{otherwise},\n\\end{cases} \\text{ and}\\] \\[\\frac{\\partial f_i}{\\partial\nb_{j}}  =\\begin{cases}\n1 &amp; \\text{ if }i = j\\\\\n0 &amp; \\text{otherwise}.\n\\end{cases}\\]\n\n\nWe can check that ∇θf(x,θ) would ‘look’ like two n × n block diagonal matrices side-by-side. The diagonal elements of the first block diagonal matrix are each given by the co-vector \\(x^T =\n\\begin{bmatrix}x_1 &amp; x_2 &amp; \\cdots &amp; x_n\n\\end{bmatrix}\\) and the second block diagonal matrix is just the identity matrix: \\[\\nabla_\\theta f(x,\\theta)\n= \\begin{bmatrix}  \\left. \\begin{matrix} x^T &amp; 0 &amp; \\cdots\n&amp;  0 \\\\ 0 &amp; x^T &amp; \\cdots &amp; 0 \\\\ \\vdots  &amp; \\vdots\n&amp; \\ddots &amp; \\vdots\\\\ 0&amp; 0 &amp; \\cdots &amp;\nx^T\\end{matrix}\\right| &amp; I_{n}\\end{bmatrix}.\\] A more compact approach would be to observe that the partial derivative written above leads to ∇Wif(x,Wi) = xT, and ∇Wjf(x,Wi) = 0 for i ≠ j.\n\n\nAnyway, we get \\[\\nabla_\\theta\nf(x,\\theta)^T = \\begin{bmatrix}   \\begin{matrix} x &amp; 0 &amp; \\cdots\n&amp;  0 \\\\ 0 &amp; x &amp; \\cdots &amp; 0 \\\\ \\vdots  &amp; \\vdots &amp;\n\\ddots &amp; \\vdots\\\\ 0&amp; 0 &amp; \\cdots &amp; x\\end{matrix} \\\\\nI_{n}\\end{bmatrix}, \\nabla_\\theta f(x,\\theta)^T \\delta =\n\\begin{bmatrix}   x \\delta_1 \\\\ x \\delta_2\\\\ \\vdots\\\\ x \\delta_n \\\\\n\\delta\\end{bmatrix}\\]\n\n\nThe update for the vector of parameters (W(row 1))T can be written as (W(row 1))T ← (W(row 1))T + δ1x, or W(row 1) ← W(row 1) + δ1xT We can stack these updates into a matrix update for W and vector update for b that would look like Wk + 1 ← Wk + δk xT, bk + 1 ← bk + δk, where δk = y − f(x,θk), and k refers to time, not an element of some vector δ.\n\n\nNonlinear Activation\n\n\nWe derive the weight updates when considering a single-layer neural network, also known as a perceptron. This network is the composition of a linear function with an element-wise application of a nonlinear activation function σ: \\[\\begin{aligned}\n  y(x,\\Theta) = \\sigma.(f(x,\\theta)) = \\sigma.(W x + b),\n\\end{aligned}\\] where σ.(⋅) indicates an element-wise application of a function σ with scalar inputs to a vector. We may look at the ith element of the output y, and write it as ŷi(x,θ) = σ(fi(x,θ)) = σ.(Wix+bi). From the chain rule, \\[\\frac{\\partial\n}{\\partial \\theta} \\hat y_i(x,\\theta) = \\left. \\frac{\\partial\n\\sigma}{\\partial z} \\right|_{ z = f_i(x,\\theta) = W_i x + b_i}\n\\frac{\\partial }{\\partial \\theta}f_i(x,\\theta).\\] Let Σ(z) be a diagonal matrix with ith entry σ(zi). Similarly, let Σ′(z) be a diagonal matrix with ith entry σ′(zi). We collect the n gradients to get ∇θŷ(x,θ) = Σ′(z)∇θf(x,θ).\n\n\nWe’ve already evaluated the second matrix in the expression above. If we pre-multiply ∇θf(x,θ) by a matrix M, then the update for W and b would become \\[\\begin{aligned}\nW_{k+1} &amp;\\gets W_{k} + M^T \\delta_k\\ x^T,\\\\\nb_{k+1} &amp;\\gets b_{k} + M^T \\delta_k,\n\\end{aligned}\\]\n\n\nNoting that Σ′(ŷ) = Σ′(ŷ)T, the W and b update would simply become \\[\\begin{aligned}\nW_{k+1} &amp;\\gets W_{k} + \\Sigma'(\\hat y) \\delta_k x^T\\\\\nb_{k+1} &amp;\\gets b_{k} + \\Sigma'(\\hat y) \\delta_k,\n\\end{aligned}\\] where δk = y − σ.(f(x,θk)). Note that σ(⋅) and σ′(⋅) are very simple for ReLu functions and sigmoids. Note that if our network consists of a single layer, we can update the neurons weights by observing the error at the output, and the input. Each node has access to the input, but we must broadcast the network error to each node. Each node’s update can be implemented ‘row-wise’, making learning local to each node.\n\n\nPerceptron\n\n\nThe perceptron update algorithm leaves the term Σ′(ŷ) out. This omission is justified for a sigmoid activation functions since σ′(a) ≥ 0 for any x, so we are merely rescaling the individual updates. However, we are no longer performing a steepest gradient descent update.\n\n\nStacking Layers\n\n\nModel With Single Hidden Layer\n\n\nNow, we add the hidden layer. ŷ = f(x,Θ) = σ.(Wo(σ.(Wix+bi))+bo) Or simply ŷ(z,θo) = σ.(Woz+bo),  z(x,θi) = σ.(Wix+bi) The updates for Wo and bo don’t change, since z is as good as a constant input no matter what the values of these parameters are: \\[\\begin{aligned}\nW_{k+1}^o &amp;\\gets W_{k}^o + \\Sigma'(W^o z + b^o) \\delta_k^o z^T,\\\\  \nb_{k+1}^o &amp;\\gets b_{k}^o + \\Sigma'(W^o z + b^o) \\delta_k^o.\n\\end{aligned}\\]\n\n\nFortunately, the gradient for θi doesn’t get too complicated. Notice that ∇θif(x,Θ) = ∇zŷ(z,θ)∇θiz(x,θi) = Wo∇θiz(x,θi)\n\n\nWe’ve seen this pattern before, and we can therefore claim that the updates for hidden-layer parameters Wi, bi – with nonlinearity accounted for – will be \\[\\begin{aligned}\n  W_{k+1}^i &amp;\\gets W_{k}^i + \\Sigma'(W^i x + b^i) \\left( W^o\n\\right)^T \\Sigma'(W^o z + b^o) \\delta_k^o x^T,\\\\\nb_{k+1}^i &amp;\\gets b_{k}^i + \\Sigma'(W^i x + b^i) \\left( W^o\n\\right)^T  \\Sigma'(W^o z + b^o)\\delta_k^o.\n\\end{aligned}\\]\n\n\nRemember that δko = y − Σ(f(x,θk)) is the full model’s prediction error using the parameters at iteration k. Define δki = Σ′(Wix+bi)(Wo)TΣ′(Woz+bo)δko. Then, the update equations for the hidden layer look like : \\[\\begin{aligned}\n  W_{k+1}^i &amp;\\gets W_{k}^i +  \\delta^i_k x^T,\\\\\nb_{k+1}^i &amp;\\gets b_{k}^i + \\delta^i_k.\n\\end{aligned}\\]\n\n\nThese equations resemble the update for a single layer model with no hidden layer. The important idea is that if this hidden layer could observe δki, it could implement a simple local rule just like the single-layer case. Error Backpropagation is precisely the task of propagating δk to δki. However, the use of (Wo)T makes backpropagation non-local to nodes, although it is local to layers.\n\n\nTo avoid using (Wo)T, some methods use a feedback layer B so that δki = Bδk. The update rule for Wo should hopefully make Wo → BT, an approach known as feedback weight alignment. Note that with this approach, all learning is performed by choosing the affine terms b for each node, since Wo → BT.\n\n\nA more generic approach is to design update rules for Wo and B so that Wo = BT.\n\n\nSimpler Single Hidden Layer\n\n\nConsider the model ŷ = f(x,Θ) = W(σ.(Hx+b)) + c The updates are then \\[\\begin{aligned}\nW_{k+1} &amp;\\gets W_{k} + \\delta_k^o z^T,\\\\  \nc_{k+1} &amp;\\gets c_{k} + \\delta_k^o.\n\\end{aligned}\\]\n\n\n\\[\\begin{aligned}\n  H_{k+1} &amp;\\gets H_{k} + \\Sigma'(H x + b) W^T  \\delta_k^o x^T,\\\\\nb_{k+1} &amp;\\gets b_{k} + \\Sigma'(H x + b) W^T  \\delta_k^o.\n\\end{aligned}\\]\n\n\nMultiple layers\n\n\nWhen we have L layers, the network function may be written as \\[\\begin{aligned}\n  y = x_L = \\sigma\\ \\circ \\mathrm{Aff}_L \\circ \\sigma\\ \\circ\n\\mathrm{Aff}_{L-1} \\circ \\sigma\\ \\circ \\mathrm{Aff}_{L-2}  \\circ \\cdots\n\\circ \\sigma\\ \\circ \\mathrm{Aff}_{2} \\circ \\sigma\\ \\circ\n\\mathrm{Aff}_{1} (x_0).\n\\end{aligned}\\] The output of layer l is xl, which becomes the input to layer l + 1. So, the first layer has input x0, the network’s input. Note that in some cases the last nonlinearity is actually identity, allowing the output to be an affine function AffL of a corresponding L − 1 layer neural network.\n\n\nThe generic update at layer l is \\[\\begin{aligned}\n  W_l &amp;\\gets W_l + \\eta \\delta_l x_{l-1}^T\\\\\n  b_l &amp;\\gets b_l + \\eta \\delta_l, \\text{ where}\\\\\n  z_l&amp; = \\mathrm{Aff}_l(x_{l-1})\n\\end{aligned}\\] δl: feedback error at layer l is given by δl = Σ′(zl)Wl + 1T⋯Σ′(zL − 3)WL − 2TΣ′(zL − 2)WL − 1TΣ′(zL − 1)WLTΣ′(zL)e. e: error at output layer e = y − xL L: number of layers xl: output of layer l xl: input to layer l + 1 η: learning rate For n = 1, NO hidden layer, we get \\[\\begin{aligned}\n\\delta_1 = \\Sigma'(z_{1}) e\n\\end{aligned}\\]\n\n\nFor n = 2, a single hidden layer, we get \\[\\begin{aligned}\n  \\delta_1 &amp;= \\Sigma'(z_{1}) W_2^T \\Sigma'(z_{2}) e,\\\\\n   \\delta_2 &amp;= \\Sigma'(z_{2}) e\n\\end{aligned}\\]\n\n\nFor n = 3, two hidden layers, we get \\[\\begin{aligned}\n\\delta_1 &amp;= \\Sigma'(z_{1}) W_2^T \\Sigma'(z_{2}) W_3^T \\Sigma'(z_{3})\ne \\\\\n\\delta_2 &amp;= \\Sigma'(z_{2}) W_3^T \\Sigma'(z_{3}) e\\\\\n\\delta_3 &amp;=  \\Sigma'(z_{3}) e\n\\end{aligned}\\]\n\n\nLyapunov loss\n\n\nConsider a model V = V(x,θ). Let g(x,θ) = V̇(x) + αV(x,θ)\n\n\nThe loss of the model is Lθ(x) = max (V̇(x)+αV(x,θ),0). Let \\[\\mathrm{posind}(x) = \\begin{cases} 1\n&amp; \\text{if } x \\geq 0  \\\\ 0 &amp;  \\text{if } x &lt; 0\n\\end{cases}\\] The partial derivative of L with respect to θ is \\[\\frac{\\partial L}{\\partial \\theta} =\n\\mathrm{posind}\\left(  g(x,\\theta)  \\right) \\frac{\\partial g}{\\partial\n\\theta}\\]\n\n\nFor ReLU NNs, we expect that at a point xi, \\[\\begin{aligned}\n  V(x_i) &amp;= p^T x_i + q_i\\\\\n  \\dot V(x_i) &amp;= p^T (A_i x_i + a_i)\\\\\n\\implies  g(x_i,\\theta ) &amp;= p^T (A_i x_i + a_i) + \\alpha  p^T x_i +\n\\alpha q_i\\\\\n&amp;= p^T (A_i + \\alpha I) x_i + p^T a_i +\\alpha q_i\n\\end{aligned}\\] Recall that θ = (Wv,Hv,bv), so that V(x,θ) = Wvσ+.(Hvx+bv) = WvD(x)Hvx + WvD(x)bv Therefore, at xi, \\[\\begin{aligned}\n  p^T &amp;=  W_v D(x_i) H_v\\\\\n  q &amp;=  W_v D(x_i) b_v\n\\end{aligned}\\]\n\n\nFinally, \\[\\begin{aligned}\n  g(x_i,\\theta ) &amp;= W_v D(x_i) H_v (A_i + \\alpha I) x_i + W_v D(x_i)\nH_v a_i +\\alpha W_v D(x_i) b_v\n\\end{aligned}\\] So, the steepest descent direction is \\[\\begin{aligned}\n  \\Delta W_v &amp;\\gets D(x_i) H_v (A_i + \\alpha I) x_i +D(x_i) H_v a_i\n+ \\alpha D(x_i) b_v  \\\\\n  \\Delta H_v &amp;\\gets D(x_i) W_v^T x_i^T (A_i^T + \\alpha I) +  D(x_i)\nW_v^T a_i^T\\\\\n  \\Delta b_v &amp;\\gets \\alpha D(x_i) W_v^T\n\\end{aligned}\\]\n\n\nIn general, for ẋ = f(xi), \\[\\begin{aligned}\ng(x_i,\\theta ) &amp;= p^T \\left( f(x_i) + \\alpha x_i \\right) + \\alpha\nq_i\\\\\n&amp;= W_v D(x_i) H_v \\left( f(x_i) + \\alpha x_i \\right) + \\alpha W_v\nD(x_i) b_v\n\\end{aligned}\\] $$\n\\[\\begin{aligned}\n  \\Delta W_v &amp;\\gets D(x_i) H_v \\left( f(x_i)+\\alpha x_i \\right) +\n\\alpha D(x_i) b_v \\\\\n  \\Delta H_v &amp;\\gets D(x_i) W_v^T \\left( f(x_i)+\\alpha x_i \\right)^T\n\\\\\n  \\Delta b_v &amp;\\gets \\alpha D(x_i) W_v^T\n  \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;h1 id=\"sec:jacobianconstraints\"&gt;Constrained Weights For Gravity\nLearning&lt;/h1&gt;\n&lt;div class=\"AsciiList\"&gt;\n&lt;p&gt;&lt;span&gt;*,-&lt;/span&gt; * Since &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; is the partial\nderivative of a potential function, its own partial derivative must be\nsymmetric. So, if &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;q&lt;/em&gt; ∈ ℝ&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;, then &lt;span\nclass=\"math inline\"&gt;$\\frac{\\partial G_1(q)}{\\partial q_2} =\n\\frac{\\partial G_2(q)}{\\partial q_1}$&lt;/span&gt; must hold. * This\nconstraint may be incorporated as a penalty term, or as ML loves to say,\nanother loss term. * ‘Flux.jl‘ does not want to take the partial\nderivative (wrt parameters) of a partial derivative (wrt input) of an\nANN. So we do it manually.&lt;/p&gt;\n&lt;/div&gt;\n&lt;h2 id=\"single-hidden-layer\"&gt;Single-Hidden Layer&lt;/h2&gt;\n&lt;p&gt;Consider a ReLu NN with one hidden layer, and linear outputs &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;y&lt;/em&gt; = &lt;em&gt;Ĝ&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;q&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sup&gt;) + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nwhere &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;) = (&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;),&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;))&lt;/span&gt;,\nand the dot in &lt;span class=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;.&lt;/span&gt; indicates an\nelement-wise map of a scalar-domain &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; function applied to a vector.&lt;/p&gt;\n&lt;p&gt;We want to minimize the additional loss term &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\nL_2(q,\\theta)&amp;= \\left( \\frac{\\partial \\hat G_1(q,\\theta)}{\\partial\nq_2} - \\frac{\\partial \\hat G_2(q,\\theta)}{\\partial q_1}  \\right)^2  \\\\\n&amp;=  \\left( e_1^T  \\frac{\\partial \\hat G(q,\\theta)}{\\partial q} e_2 -\ne_2^T  \\frac{\\partial \\hat G(q,\\theta)}{\\partial q} e_1\\right)^2\\\\\n&amp; =  \\left(e_1^T J(q,\\theta) e_2 - e_2^T J(q,\\theta) e_1\\right)^2 \\\\\n&amp; = \\delta(q,\\theta)^2,\n\\end{aligned}\\]\n\\[&lt;/span&gt; where &lt;span\nclass=\"math inline\"&gt;{&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;}&lt;/span&gt; is the\nstandard basis of &lt;span\nclass=\"math inline\"&gt;ℝ&lt;sup&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; Let’s name some\nintermediate terms: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\np &amp;= W^i q + b^i &amp;  z &amp;= \\sigma.(p)  \\\\\nv &amp;= W^h z + b^h &amp;  y &amp;= \\sigma.(v)  \\\\\ny_o &amp;= W^o v + b^o &amp;  &amp; \\\\\n\\end{aligned}\\]\n\\[&lt;/span&gt; and note that &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;a&lt;/em&gt;)&lt;/span&gt; is a diagonal matrix\nwhose &lt;span class=\"math inline\"&gt;&lt;em&gt;i&lt;/em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt; element is\n&lt;span\nclass=\"math inline\"&gt;∂&lt;em&gt;σ&lt;/em&gt;/∂&lt;em&gt;x&lt;/em&gt;(&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;,\nwhere &lt;span class=\"math inline\"&gt;&lt;em&gt;a&lt;/em&gt;&lt;/span&gt; is a vector. The\nJacobian &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; is then\n&lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  J(q,\\theta) &amp;= \\frac{\\partial \\hat G(q,\\theta)}{\\partial q}  = W^o\n\\Sigma'(v) W^h \\Sigma'(p) W^i \\label{eq:jacobianexpression}\\\\\n  \\implies  \\delta(q,\\theta) &amp;= e_1^T W^o \\Sigma'(v) W^h \\Sigma'(p)\nW^i e_2 - e_2^T W^o \\Sigma'(v) W^h \\Sigma'(p) W^i e_1\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;To optimize &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt;\nwrt &lt;span class=\"math inline\"&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt;, we need the gradient of\n&lt;span class=\"math inline\"&gt;&lt;em&gt;δ&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; wrt\n&lt;span class=\"math inline\"&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt; at &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;. This gradient, when &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; is a ReLu function, is (skipping a\nfactor of 2): &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\frac{\\partial \\delta(q,\\theta)}{\\partial W^i} &amp;= \\left( W^o\n\\Sigma'(v) W^h \\Sigma'(p)\\right)^T \\left(e_1 e_2^T - e_2\ne_1^T  \\right)  \\\\\n  &amp;= \\Sigma'(p) (W^h)^T \\Sigma'(v) (W^o)^T \\left(e_1 e_2^T - e_2\ne_1^T  \\right)  \\\\\n\\frac{\\partial \\delta(q,\\theta)}{\\partial W^h} &amp;=  \\left( W^o\n\\Sigma'(v) \\right)^T \\left(e_1 e_2^T - e_2 e_1^T  \\right)\n\\left(   \\Sigma'(p) W^i\\right)^T \\\\\n  &amp;= \\Sigma'(v) (W^o)^T \\left(e_1 e_2^T - e_2 e_1^T  \\right)(W^i)^T\n\\Sigma'(p)   \\\\\n\\frac{\\partial \\delta(q,\\theta)}{\\partial W^o} &amp;= \\left(e_1 e_2^T -\ne_2 e_1^T  \\right) \\left( \\Sigma'(v) W^h \\Sigma'(p) W^i\\right)^T \\\\\n  &amp;= \\left(e_1 e_2^T - e_2 e_1^T  \\right)(W^i)^T \\Sigma'(p) (W^h)^T\n\\Sigma'(v)  \\\\\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;h2 id=\"single-layer-architecture\"&gt;Single-Layer Architecture&lt;/h2&gt;\n&lt;p&gt;The partial derivative expression in &lt;a href=\"#eq:jacobianexpression\"\ndata-reference-type=\"eqref\"\ndata-reference=\"eq:jacobianexpression\"&gt;[eq:jacobianexpression]&lt;/a&gt;\nassumes a single hidden layer. When evaluating &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; at a points, the\nmatrix &lt;span class=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(⋅)&lt;/span&gt; depends on &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;, and has binary-valued diagonal\nelements. Due to the nature of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(⋅)&lt;/span&gt;, we do not expect the Jacobian\nto be symmetric at most points.&lt;/p&gt;\n&lt;p&gt;One way to ensure that the Jacobian is symmetric at all points by\nconstruction is to remove the hidden layer: &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\ny=\\hat G(q,\\theta) = W^o \\sigma.\\left( W^i q + b^i \\right)  + b^o  \n\\end{aligned}\\]\n\\[&lt;/span&gt; The Jacobian &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; is given by &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  J(q)&amp;= W^o \\Sigma'(z) W^i ,\\text{ where}\\\\\n  z &amp;= W^i q + b^i\n\\end{aligned}\\]\n\\[&lt;/span&gt; If we set &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nthen at every possible input point &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;, the Jacobian is symmetric!\nInstead of directly setting these two matrices to be mutual transposes,\nwe may define an additional loss functions that either penalizes\nasymmetry of the Jacobian, or mutual asymmetry of the weight matrices.\nTheses losses and their weight updates are shown in Table &lt;a\nhref=\"#tab:lossgradients\" data-reference-type=\"ref\"\ndata-reference=\"tab:lossgradients\"&gt;1&lt;/a&gt;. The derivation of these\nexpressions are in the subsequent sections.&lt;/p&gt;\n&lt;h4 id=\"relaxed-condition.\"&gt;Relaxed condition.&lt;/h4&gt;\n&lt;p&gt;Instead of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nwe just need to find a vector &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt; such that &lt;span\nclass=\"math display\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = diag(&lt;em&gt;k&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;div id=\"tab:lossgradients\"&gt;\n&lt;table&gt;\n&lt;caption&gt;Weight matrix updates in single-layer network for different\nlosses. Note that &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;z&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.\n&lt;/caption&gt;\n&lt;thead&gt;\n&lt;tr class=\"header\"&gt;\n&lt;th style=\"text-align: left;\"&gt;Loss&lt;/th&gt;\n&lt;th style=\"text-align: center;\"&gt;Function (&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;&lt;/th&gt;\n&lt;th style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Δ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; =  − (∇&lt;sub&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;))&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;\n&lt;th style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Δ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;) =  − ∇&lt;sub&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td style=\"text-align: left;\"&gt;Training&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span class=\"math inline\"&gt;$\\frac{1}{2}\n\\lVert \\delta \\rVert^2$&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td style=\"text-align: left;\"&gt;Direct Jacobian&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;$\\frac{1}{2}\\lVert J - J^T \\rVert_F^2$&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;−&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;−&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td style=\"text-align: left;\"&gt;Indirect Jacobian&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;$\\frac{1}{2}\\lVert W^i - \\left( W^o\n\\right)^T\\rVert_F^2$&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; − (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; − &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/div&gt;\n&lt;h3 id=\"training-loss\"&gt;Training Loss&lt;/h3&gt;\n&lt;p&gt;Consider the single-layer network with output weights: &lt;span\nclass=\"math display\"&gt;&lt;em&gt;g&lt;/em&gt; = &lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;,&lt;em&gt;Θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)) + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;In an expanded form, we may write &lt;span\nclass=\"math display\"&gt;&lt;em&gt;g&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;z&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;,  &lt;em&gt;z&lt;/em&gt; = &lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;y&lt;/em&gt;),  &lt;em&gt;y&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;The updates for &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; don’t change,\nsince &lt;span class=\"math inline\"&gt;&lt;em&gt;z&lt;/em&gt;&lt;/span&gt; is as good as a\nconstant input no matter what the values of these parameters are: &lt;span\nclass=\"math display\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;,&lt;/span&gt;\n&lt;span\nclass=\"math display\"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;The updates for hidden-layer parameters &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;, &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;\n– with nonlinearity accounted for – will be &lt;span\nclass=\"math display\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)((&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;)&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;,&lt;/span&gt;\n&lt;span\nclass=\"math display\"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;.&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Remember that &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;g&lt;/em&gt; − &lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;))&lt;/span&gt;\nis the full model’s prediction error using the parameters at iteration\n&lt;span class=\"math inline\"&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;\n&lt;p&gt;So, the updates are &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\underbrace{\\Delta W^o}_{\\mathbb{R}^{n \\times m}} =\n\\underbrace{\\delta_k}_{\\mathbb{R}^{n \\times 1}}\n\\underbrace{z^T}_{\\mathbb{R}^{1 \\times m}}, \\quad \\underbrace{\\Delta\nW^i}_{\\mathbb{R}^{m \\times n}} =\\underbrace{\\Sigma'(y)}_{\\mathbb{R}^{m\n\\times m}} \\underbrace{(W^o)^T}_{\\mathbb{R}^{m \\times n}}\n\\underbrace{\\delta_k x^T}_{\\mathbb{R}^{n \\times n}}\n\\end{aligned}\\]\n\\[&lt;/span&gt; However, note that &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;z&lt;/em&gt; = &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;) ⟹ &lt;em&gt;z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; = ((&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;+&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)&lt;/span&gt;,\nso that &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  {\\Delta W^o} =  {\\delta_k (b^i)^T \\Sigma'(y)+ \\delta_k x^T(W^i)^T }\n\\Sigma'(y), \\quad {\\Delta W^i} ={\\Sigma'(y)}{(W^o)^T}{\\delta_k x^T}\n\\end{aligned}\\]\n\\[&lt;/span&gt; and &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\left( \\Delta W^o \\right)^T =  \\Sigma'(y) b^i \\delta_k^T + \\Sigma'(y)\nW^i \\left( \\delta_k x^T \\right)^T  , \\quad {\\Delta W^i}\n={\\Sigma'(y)}{(W^o)^T}{\\delta_k x^T}\n\\end{aligned}\\]\n\\[&lt;/span&gt; We may take the difference to obtain &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\left( \\Delta W^o \\right)^T - \\Delta W^i = \\Sigma'(y) b^i \\delta_k^T +\n\\Sigma'(y) \\left(   W^i  Z^T - (W^o)^T  Z   \\right),\n\\end{aligned}\\]\n\\[&lt;/span&gt; where &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.\nLet &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.\nThen, &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\left( \\Delta W^o \\right)^T - \\Delta W^i = \\Sigma'(y) b^i \\delta_k^T +\n\\Sigma'(y) \\left(   W^i  - (W^o)^T     \\right) Z^{sym} + \\Sigma'(y)\n\\left(   W^i  + (W^o)^T     \\right) Z^{asym},\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;When &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = 0&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nthe loss gradient would cause updates in a way that reduces the\ndifference between &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nwhich would make the Jacobian symmetric.&lt;/p&gt;\n&lt;p&gt;We may derive this expression in another way. Note that &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;z&lt;/em&gt;) = &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;z&lt;/em&gt;&lt;/span&gt;.\nTherefore, &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\ng &amp;= f(x,\\Theta) = W^o (\\sigma.(W^i x + b^i)   ) + b^o \\\\\n&amp;= W^o \\Sigma'(z) W^i x + W^o \\Sigma'(z) b^i + b^o\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Define &lt;span class=\"math inline\"&gt;$L = \\frac{1}{2}\\lVert\n\\delta\\rVert^2 =  \\sum_j \\frac{1}{2} \\delta_j^2$&lt;/span&gt;. The gradient\nw.r.t. &lt;span class=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;\nis &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n\\frac{\\partial L}{\\partial W^o} &amp;= \\frac{\\partial L}{\\partial\n\\delta}\\frac{\\partial \\delta}{\\partial W^o} =   \\sum_j\n\\delta_j  \\frac{\\partial \\delta_j}{\\partial W^o}=   \\sum_j\n\\delta_j  \\frac{\\partial (-f_j(x,\\theta))}{\\partial W^o}\\\\\n&amp;=- \\sum_j \\delta_j \\frac{\\partial \\left( e_j^T   W^o \\Sigma'(z) W^i\nx + e_j^T  W^o \\Sigma'(z) b^i  \\right)}{\\partial W^o}\\\\\n&amp;=  - \\sum_j \\delta_j e_j \\left( x^T \\left( W^i \\right)^T\n\\Sigma'(z)  \\right)- \\sum_j \\delta_j e_j \\left( \\left( b^i \\right)^T\n\\Sigma'(z) \\right) \\\\\n&amp;= - \\delta x^T \\left( W^i \\right)^T \\Sigma'(z) - \\delta \\left( b^i\n\\right)^T \\Sigma'(z)\n\\end{aligned}\\]\n\\[&lt;/span&gt; Similarly, we may calculate &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\frac{\\partial L}{\\partial W^i} &amp;= -\\sum_j \\delta_j   \\left(\n\\Sigma'(z)  \\left( W^o \\right)^T e_j x^T  \\right) \\\\\n  &amp;= -\\Sigma'(z)  \\left( W^o \\right)^T \\delta x^T\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;h3 id=\"direct-jacobian-loss\"&gt;Direct Jacobian Loss&lt;/h3&gt;\n&lt;p&gt;If &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;Θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nthen the Jacobian loss &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;J&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;\nis given by &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\nL_J(\\Theta) &amp;= e_1^T W^o \\Sigma'(y) W^i e_2 - e_2^T W^o \\Sigma'(y)\nW^i e_1,\n\\end{aligned}\\]\n\\[&lt;/span&gt; where &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;y&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;q&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.\nSimilar to the hidden layer case, we derive the gradient of this\nfunction with respect to the network weights. &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\frac{\\partial L_J(\\Theta)}{\\partial W^o} &amp;= e_1 e_2^T \\left( W^i\n\\right)^T \\Sigma'(y) - e_2 e_1^T \\left( W^i \\right)^T\\Sigma'(y) = \\left(\ne_1 e_2^T - e_2 e_1^T \\right) \\left( W^i \\right)^T\\Sigma'(y)\\\\\n    \\frac{\\partial L_J(\\Theta)}{\\partial W^i} &amp;= \\Sigma'(y)  \\left(\nW^o \\right)^T e_1 e_2^T - \\Sigma'(y)  \\left( W^o \\right)^T e_2 e_1^T =\n\\Sigma'(y)  \\left( W^o \\right)^T \\left( e_1 e_2^T - e_2 e_1^T \\right)\n\\end{aligned}\\]\n\\[&lt;/span&gt; Together, we get that &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\frac{\\partial L_J(\\Theta)}{\\partial W^o} ^T &amp;= - \\Sigma'(y) W^i\n\\left( e_1 e_2^T - e_2 e_1^T \\right) \\\\\n  \\frac{\\partial L_J(\\Theta)}{\\partial W^i} &amp; = \\Sigma'(y) \\left(\nW^o \\right)^T \\left( e_1 e_2^T - e_2 e_1^T \\right)\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;We may write &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;J&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;\nas &lt;span class=\"math display\"&gt;\\]L_J() = {j=1}^m ( w^o{1j} ‘(z_j)w^i_{j2} - w^o_{2j} ’(z_j)w^i_{j1} ).\\[&lt;/span&gt; So, for\nsymmetry, we don’t need &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.\nInstead, we need that on average, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;(:,&lt;em&gt;j&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;j&lt;/em&gt;,:)&lt;/span&gt;\nis symmetric.&lt;/p&gt;\n&lt;h3 id=\"indirect-jacobian-loss\"&gt;Indirect Jacobian Loss&lt;/h3&gt;\n&lt;p&gt;This section derives the gradient of the indirect loss on the weights\nof the neural network. By indirect, we mean that the goal is to obtain a\nsymmetric Jacobian, and we define a loss function directly on the\nweights that indirectly achieves a symmetric Jacobian. It only works for\na single layer network. The loss is given by &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  L(J(q,\\Theta)) = L(W^o,W^i)  = \\lVert W^i - \\left( W^o\n\\right)^T\\rVert_F^2\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;The Frobenius norm of a (real-valued) matrix &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/span&gt; is &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\lVert M  \\rVert_F = \\sqrt{ \\mathrm{tr\\ }M M^T}\n\\end{aligned}\\]\n\\[&lt;/span&gt; As shown in &lt;a\nhref=\"https://web.stanford.edu/~jduchi/projects/matrix_prop.pdf\"&gt;these\nnotes&lt;/a&gt;, &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\nabla_A \\mathrm{tr\\ } A B &amp;= B^T \\\\\n  \\nabla_A \\mathrm{tr\\ } A B A^T C &amp;= C A B + C^T A B^T\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;If &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;M&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; − (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,\nthen &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n\\mathrm{tr\\ }M M^T  &amp;= \\mathrm{tr\\ }\\left( W^i - \\left( W^o\n\\right)^T \\right) \\left( \\left( W^i \\right)^T -  W^o  \\right) \\\\\n&amp;= \\mathrm{tr\\ }W^i\\left( W^i \\right)^T  - \\mathrm{tr\\ }W^i W^o  -\n\\mathrm{tr\\ }\\left( W^o \\right)^T  \\left( W^i \\right)^T +\\mathrm{tr\\\n}\\left( W^o \\right)^T  W^o\\\\\n&amp;= \\mathrm{tr\\ }W^i\\left( W^i \\right)^T + \\mathrm{tr\\ }W^o\\left( W^o\n\\right)^T - 2 \\mathrm{tr\\ }W^i W^o\n\\end{aligned}\\]\nW^i W^o \\end{aligned}\\[&lt;/span&gt; We have used the following facts: the trace is\nlinear, the trace is invariant under transposition (adjoint map), and\nthe trace is invariant under cyclic permutation of matrix products.&lt;/p&gt;\n&lt;p&gt;With these details in place, we derive &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\nabla_{W^o} L(W^o,W^i) &amp;=   \\nabla_{W^o} \\mathrm{tr\\ }  \\left(\nW^i - \\left( W^o \\right)^T \\right) \\left( \\left( W^i \\right)^T\n-  W^o  \\right) \\\\\n  &amp;= \\nabla_{W^o} \\left( \\mathrm{tr\\ }W^i\\left( W^i \\right)^T +\n\\mathrm{tr\\ }W^o\\left( W^o \\right)^T - 2 \\mathrm{tr\\ }W^i W^o \\right)\\\\\n  &amp;=2 \\left( W^o- \\left( W^i \\right)^T \\right), \\text{ and}\\\\\n\\nabla_{W^i} L(W^o,W^i) &amp;=  2\\left( W^i- \\left( W^o \\right)^T\n\\right)\n\\end{aligned}\\]\n\\[&lt;/span&gt; The negative gradient of this loss function\nbrings &lt;span class=\"math inline\"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;\nand &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;\ncloser to together.&lt;/p&gt;\n&lt;h2 id=\"ideas-from-lyapunov-work\"&gt;Ideas from Lyapunov Work&lt;/h2&gt;\n&lt;p&gt;We have been viewing a single-layer ReLU Network as the piecewise\naffine map &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  \\Sigma'(z) H x +  \\Sigma'(z) b  \\geq 0 \\implies f(x) = W (\\Sigma'(z) H\nx +  \\Sigma'(z) b)\n\\end{aligned}\\]\n\\[&lt;/span&gt; We wanted &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;) ≤ 0&lt;/span&gt; when &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;)&lt;/span&gt; was a scalar, and we\nused theorems of alternatives to eliminate &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;x&lt;/em&gt;&lt;/span&gt;. The idea we are exploring is that\non each cell, we need to solve a linear regression problem. This may run\ninto the whole reason for networks+SGD: you can’t handle millions of\ndata.&lt;/p&gt;\n&lt;h1 id=\"hessian-descent-for-gravity-learning\"&gt;Hessian Descent For\nGravity Learning&lt;/h1&gt;\n&lt;p&gt;In Section &lt;a href=\"#sec:jacobianconstraints\"\ndata-reference-type=\"ref\"\ndata-reference=\"sec:jacobianconstraints\"&gt;5&lt;/a&gt;, the neural network\nlearns &lt;span class=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;, and we\nattempt to constrain the Jacobian of this map to bias the learning. This\nbiasing does not appear work. One reason may be Stefano Soatto’s &lt;a\nhref=\"https://sites.google.com/view/control-meets-learning/home/past-seminars\"&gt;talk&lt;/a&gt;\nabout how there appears to be an intermediate memorization phase before\na generalization phase, corresponding to escaping a rough loss landscape\nbefore arriving at a relatively flat basin. Applying that idea here, we\nneed to memorize &lt;span class=\"math inline\"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;\nwithout worrying about physics, before applying physics-based\nconstraints such as symmetry of the Jacobian while training. Note that\nin &lt;span&gt;&lt;code&gt;Julia&lt;/code&gt;&lt;/span&gt; experiments on the\n&lt;code&gt;RigidBodyDynamics&lt;/code&gt; acrobot model, the original loss has to\nbecome &lt;span class=\"math inline\"&gt;𝒪(10&lt;sup&gt;−4&lt;/sup&gt;)&lt;/span&gt; before seeing\nsymmetry, a loss of &lt;span class=\"math inline\"&gt;𝒪(10&lt;sup&gt;−2&lt;/sup&gt;)&lt;/span&gt;\nis not enough. This observation suggests a hypothesis that\nsymmetrization occurs in later stages of training.&lt;/p&gt;\n&lt;p&gt;We now investigate an alternate strategy, in which the network is\nmeant to predict the potential energy &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;, where &lt;span\nclass=\"math inline\"&gt;$\\frac{\\partial P}{\\partial q} = G(q)$&lt;/span&gt;. We\nnow know that any ReLU network has a partial derivative of the form\n&lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n  J(q) = W_L \\Sigma_{L-1}' \\left( z_{L-1} \\right) W_{L-1}\\Sigma_{L-2}'\n\\left( z_{L-2} \\right) W_{L-2} \\cdots W_1\n\\end{aligned}\\]\n$$\n\n\nIf our network predicts P(q), then we have a piecewise linear map G(q), with discontinuities at the pieces. In other words, this is a bad idea when using ReLU.\n\n\nFeedback Alignment\n\n\nWith the above background, we may begin to investigate network training approaches based on Feedback Alignment. The idea is that back propagation is not biologically plausible because of the reliance on non-local information. Feedback Alignment is a local update rule based on the idea that a network can learn – using local rules – to make the forward pass mimic a static backprop error term, thereby achieving a more biologically plausible backpropogation step.\n\n\nWhat does DFA optimize?\n\n\nThe desire to achieve alignment is so that the updates optimize ∥e∥2/2. Two questions remain unanswered: 1. using the normal updates, why does W → BT ? Or is the claim untrue outside specific conditions? Presentation you found suggests that there’s no proof backing the empirical performance on deep networks. Found cases where loss is not getting worse as time goes on. 2. Is there a function for which the DFA updates is the currect update?\n\n\nLearning to Learn with Feedback and Local Plasticity\n\n\n\n\nThe idea of this paper is to use meta-learning to choose the initial weights of a network, and the feedback matrices. %\n\n\n\n\nThen, learning is through hebbian synaptic pasticity using Oja’s learning rule:w ← w + α(ab−b2w), where the presynaptic activity is a, and the postsynaptic activity resulting from feedback is b\n\n\n\n\nOja’s learning rule is an approximation to normalized Hebbian learning, and he show’s that if α varies in a specific way (eg. 1/t), the network weights will align with the principal component of the input signal. This proof uses some stochastic approximation theory that leads to an ODE.\n\n\n\n\n\n\nFeedback is through a linear map but is NOT of the form δi = Bi(y−ŷ). For some reason, modified activations are used in the updates: xi ← (1−βi)xi + βiReLU(Biy−b)"
  },
  {
    "objectID": "simplemodels.html",
    "href": "simplemodels.html",
    "title": "Simple Models",
    "section": "",
    "text": "Suppose we have noisy data \\((x_i,y_i)_{1,\\dots,N}\\) that corresponds to a true function \\(f(x)= 4 x + 5\\). It looks like:\n\n\n\n\n\n\n\n\n\nOur goal is to identify \\(f(x)\\) from data \\((x_i,y_i)_{1,\\dots,N}\\).\nWe can cheat a bit and claim that the relationship is given by a linear model: \\[ f(x) = a x +b.\\]\nGiven a datum, say \\((x_2,y_2)\\), our guess for \\(f(x_2)\\) is \\(a x_2 +b\\). If we know that \\(a=4\\) and \\(b=5\\), we would get \\(y_2 = f(x_2)\\) (ignoring noise). If we don’t have a good guess, then \\(f(x_2)\\) may not be close to \\(y_2\\):\n\n\n\n\n\n\n\n\n\nThe most used method for identifying, or learning, \\(a\\) and \\(b\\) is precisely to find values for those parameters that minimize the average (squared) error between \\(y_i\\) and \\(f(x_i) = a x_i + b\\). Let the error for \\((x_i,y_i)\\) be \\(e_i = y_i - a x_i - b\\). We want to minimize \\[ \\frac{1}{N} \\sum_{i=1}^N e_i^2. \\]\nThis expression is called the Loss function. It is a measure of how bad the guess for \\(a\\) and \\(b\\) are. \\[ L(a,b) = \\frac{1}{N} \\sum_{i=1}^N e_i(a,b)^2, \\] where we now make the dependence of \\(e_i\\) on \\(a\\) and \\(b\\) explicit. We can combine all the scalar values \\(e_i\\) into the vector \\(\\mathbf{e}\\): \\[\\mathbf{e} = \\begin{bmatrix}e_1\\\\e_2\\\\e_3\\\\ \\vdots\\\\e_N\\end{bmatrix} = \\begin{bmatrix}y_1 - a x_1 - b\\\\y_2 - a x_2 - b\\\\y_3 - a x_3 - b\\\\ \\vdots\\\\y_N - a x_N - b\\end{bmatrix} = \\mathbf{y} - \\begin{bmatrix} \\mathbf{x} & \\mathbf{1} \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\mathbf{y} - \\mathbf{X} \\theta.\\] In fact, we did a lot of combining scalars into vectors and matrices. It allows us to rewrite the Loss function:\n\\[ L(a,b) = L(\\theta) = \\frac{1}{N} \\mathbf{e}^\\top \\mathbf{e}. \\]\nWe want to find \\(\\theta = [a;b]\\) that minimizes this function. Our learning problem is now an unconstrained optimization problem with a differentiable objective.\n\\[ \\theta^\\star = \\arg \\min_\\theta L(\\theta). \\]\nThe theory for this objective function is especially nice so we can derive a closed-form solution for the minimizer:\n\\[ \\theta^\\star = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top  \\mathbf{y}.\\]\n\n## Closed-form solution compare prediction with true model\nXt = np.stack([x,np.ones(100)])  # build transpose of X \ntheta =  np.linalg.inv(Xt @ Xt.T) @ (Xt @ y.T ) # Implement the equation above using numpy (np)\nprint(f\"a = {theta[0]}, b = {theta[1]}\")\ny_opt = theta[0]*x + theta[1] # prediction using these optimal parameters\nplt.figure(figsize=(8, 6))\n# plot data, noise free, prediction:\nplt.scatter(x,y,label=\"data\")\nplt.plot(x,y_line,label=\"noise free\",linestyle=\"dashed\")\nplt.plot(x,y_opt,label=f\"prediction f(x) = a x + b\")\nplt.xlabel(f\"$x$\")\nplt.ylabel(\"$y$\")\nplt.grid(True)\n\nplt.legend()\nplt.show()\n\na = 3.912437593039594, b = 4.998100325639219\n\n\n\n\n\n\n\n\n\n\n\nWe found \\(\\theta = [a;b]\\) by solving the unconstrained minimization problem in closed form. An alternative is to use an iterative algorithm that tries to pick a new guess for \\(\\theta\\) that has a lower loss. For differentiable objectives, one algorithm is steepest descent:\n\\[\\theta_{k+1} = \\theta_{k} - \\alpha  \\nabla L(\\theta_k),\\]\nwhere \\(\\alpha\\) is a step size and \\(\\nabla L\\) is the gradient of the Loss function.\nThe iterates of \\(\\theta\\) and the loss value are given below.\n\n\na = 4.006384632625632, b = 5.005149393190814",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Models"
    ]
  },
  {
    "objectID": "simplemodels.html#linear-models-regression",
    "href": "simplemodels.html#linear-models-regression",
    "title": "Simple Models",
    "section": "",
    "text": "Suppose we have noisy data \\((x_i,y_i)_{1,\\dots,N}\\) that corresponds to a true function \\(f(x)= 4 x + 5\\). It looks like:\n\n\n\n\n\n\n\n\n\nOur goal is to identify \\(f(x)\\) from data \\((x_i,y_i)_{1,\\dots,N}\\).\nWe can cheat a bit and claim that the relationship is given by a linear model: \\[ f(x) = a x +b.\\]\nGiven a datum, say \\((x_2,y_2)\\), our guess for \\(f(x_2)\\) is \\(a x_2 +b\\). If we know that \\(a=4\\) and \\(b=5\\), we would get \\(y_2 = f(x_2)\\) (ignoring noise). If we don’t have a good guess, then \\(f(x_2)\\) may not be close to \\(y_2\\):\n\n\n\n\n\n\n\n\n\nThe most used method for identifying, or learning, \\(a\\) and \\(b\\) is precisely to find values for those parameters that minimize the average (squared) error between \\(y_i\\) and \\(f(x_i) = a x_i + b\\). Let the error for \\((x_i,y_i)\\) be \\(e_i = y_i - a x_i - b\\). We want to minimize \\[ \\frac{1}{N} \\sum_{i=1}^N e_i^2. \\]\nThis expression is called the Loss function. It is a measure of how bad the guess for \\(a\\) and \\(b\\) are. \\[ L(a,b) = \\frac{1}{N} \\sum_{i=1}^N e_i(a,b)^2, \\] where we now make the dependence of \\(e_i\\) on \\(a\\) and \\(b\\) explicit. We can combine all the scalar values \\(e_i\\) into the vector \\(\\mathbf{e}\\): \\[\\mathbf{e} = \\begin{bmatrix}e_1\\\\e_2\\\\e_3\\\\ \\vdots\\\\e_N\\end{bmatrix} = \\begin{bmatrix}y_1 - a x_1 - b\\\\y_2 - a x_2 - b\\\\y_3 - a x_3 - b\\\\ \\vdots\\\\y_N - a x_N - b\\end{bmatrix} = \\mathbf{y} - \\begin{bmatrix} \\mathbf{x} & \\mathbf{1} \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\end{bmatrix} = \\mathbf{y} - \\mathbf{X} \\theta.\\] In fact, we did a lot of combining scalars into vectors and matrices. It allows us to rewrite the Loss function:\n\\[ L(a,b) = L(\\theta) = \\frac{1}{N} \\mathbf{e}^\\top \\mathbf{e}. \\]\nWe want to find \\(\\theta = [a;b]\\) that minimizes this function. Our learning problem is now an unconstrained optimization problem with a differentiable objective.\n\\[ \\theta^\\star = \\arg \\min_\\theta L(\\theta). \\]\nThe theory for this objective function is especially nice so we can derive a closed-form solution for the minimizer:\n\\[ \\theta^\\star = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\top  \\mathbf{y}.\\]\n\n## Closed-form solution compare prediction with true model\nXt = np.stack([x,np.ones(100)])  # build transpose of X \ntheta =  np.linalg.inv(Xt @ Xt.T) @ (Xt @ y.T ) # Implement the equation above using numpy (np)\nprint(f\"a = {theta[0]}, b = {theta[1]}\")\ny_opt = theta[0]*x + theta[1] # prediction using these optimal parameters\nplt.figure(figsize=(8, 6))\n# plot data, noise free, prediction:\nplt.scatter(x,y,label=\"data\")\nplt.plot(x,y_line,label=\"noise free\",linestyle=\"dashed\")\nplt.plot(x,y_opt,label=f\"prediction f(x) = a x + b\")\nplt.xlabel(f\"$x$\")\nplt.ylabel(\"$y$\")\nplt.grid(True)\n\nplt.legend()\nplt.show()\n\na = 3.912437593039594, b = 4.998100325639219\n\n\n\n\n\n\n\n\n\n\n\nWe found \\(\\theta = [a;b]\\) by solving the unconstrained minimization problem in closed form. An alternative is to use an iterative algorithm that tries to pick a new guess for \\(\\theta\\) that has a lower loss. For differentiable objectives, one algorithm is steepest descent:\n\\[\\theta_{k+1} = \\theta_{k} - \\alpha  \\nabla L(\\theta_k),\\]\nwhere \\(\\alpha\\) is a step size and \\(\\nabla L\\) is the gradient of the Loss function.\nThe iterates of \\(\\theta\\) and the loss value are given below.\n\n\na = 4.006384632625632, b = 5.005149393190814",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Models"
    ]
  },
  {
    "objectID": "simplemodels.html#logistic-regression",
    "href": "simplemodels.html#logistic-regression",
    "title": "Simple Models",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nRegression tries to find a relationship between two continuous variables. In many problems, the input to a model may be continuous, but the output we want is discrete valued. For example, our body temperature is continuous valued, but we may want to use it to decide whether to call in sick or not.\nWe can still use continuous regression, but with the trick that outputs are integer valued, and our models are biased to producing nearly-integral outputs for most inputs.\nFor example, consider the standard logistic function: \\[f(x) = \\frac{1}{1+e^{-x}}.\\] Away from \\(x=0\\), the value is close to either \\(0\\) or \\(1\\). This number is often interpreted as a probability of belonging to a class.\n\n\n\n\n\n\n\n\n\n\nPartial Iris Classification\nThe full classification tries to distinguish between three species of Irises:\n\nsetosa\nversicolor\nvirginica\n\nbased on four measurements collected by Ronald Fisher:\n\nsepal length\nsepal width\npetal length\npetal width\n\nHere, we will solve a partial version of this classification problem. We will try to predict whether an iris belongs to species setosa or versicolor from the petal length. The data look like:\n\n\n\n\n\n\n\n\n\nWe assign distinct integer variables to the two classes, namely \\(0\\) and \\(1\\):\n\n\n\n\n\n\n\n\n\nThe data suggest we can use an \\(S\\)-shaped curve to map petal length to a number that we interpret as the probability of the petal belonging to species versicolor.\nThe model is \\[ p(\\text{versicolor} | x) = \\frac{1}{1 + e^\\left( a x + b\\right)}. \\]\nWe have data \\((x_i,y_i)\\) where \\(x_i\\) is the petal length of the \\(i^{\\mathrm{th}}\\) flower and \\(y_i\\) is \\(0\\) if it is of species setosa and \\(1\\) if it is of species versicolor. Then, we can learn parameters \\(a\\) and \\(b\\) by minimizing a mean squared error loss function using gradient descent:\n\n\na = -3.2454446383309143, b = -8.2278103097029\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe prediction for all inputs overlaying the data:\n\n\n\n\n\n\n\n\n\n\n\nMore Features\nWhat if we use all four measurements (input features) to do the same classification task?\n\\[ p(\\text{versicolor} | x) = \\frac{1}{1 + e^\\left( w^\\top x + b\\right)}, \\]\nwhere \\(w \\in \\mathbb R^4\\).\nThe mean squared error loss is not convex in the parameters. We use gradient descent to optimize this function.\n\n\nw = [ 0.47315506  1.7278686  -2.66159651 -1.20674271], b = -0.30153672562670286\n\n\n\n\n\n\n\n\n\nConfusion Matrix:\n [[50  0]\n [ 0 50]]\n\n\nNote that a confusion matrix \\(C\\) is such that \\(C_{i,j}\\) is equal to the number of observations known to be in group \\(i\\) and predicted to be in group \\(j\\).\nYou can see that this classifier works perfectly on the data. This result is unsurprising. Take a look at the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can draw lines that divide setosa and versicolor using just two features, never mind all four at the same time. This property means that our model was destined to succeed perfectly for this partial classification. What do you expect will happen if we wanted to distinguish between versicolor and virginica? Would this model distinguish between all three?\nLet’s try training a classifier to distinguish between versicolor and virginica. We get a solution:\n\n\nw = [ 3.83121395  4.19340142 -6.45287598 -7.70857748], b = -8.566849954768172\n\n\n\n\n\n\n\n\n\nConfusion Matrix:\n [[48  2]\n [ 0 50]]\n\n\nThe results are reasonably good. Let’s look at the code. We load data, define a loss function which contains the model, define a gradient, then train , and finally evaluate the model. This pattern is fairly common.\n\n## Custom gradient descent to solve versicolor vs virginica classification using only four feature\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndef load_two_species(species_pair=(0, 1), *, remap=True):\n    if len(species_pair) != 2:\n        raise ValueError(\"species_pair must contain exactly two integers (e.g. (0, 2)).\")\n    if not all(label in {0, 1, 2} for label in species_pair):\n        raise ValueError(\"Labels must be chosen from 0, 1, 2.\")\n\n    iris = load_iris()\n    X_all, y_all = iris.data, iris.target\n\n    # Boolean mask: keep rows whose label is in species_pair\n    mask = np.isin(y_all, species_pair)\n    X, y = X_all[mask], y_all[mask]\n\n    if remap:\n        # Map the first chosen label → 0, the second → 1\n        label_map = {species_pair[0]: 0, species_pair[1]: 1}\n        y = np.vectorize(label_map.get)(y)\n\n    return X, y\n\n\n# Load the dataset to get names\niris = load_iris()\n\n# Access the data, only two species\niris_species_1 = 1\niris_species_2 = 2\nX, y = load_two_species((iris_species_1, iris_species_2))\n\n# Define the objective function and its gradient \ndef L(w, b):\n    e = y - 1/(1+np.exp(X @ w-b*np.ones(100)))\n    return np.linalg.norm(e)/100\n\ndef grad_L(w, b):\n    \"\"\"Gradient of f\"\"\"\n    e = y - 1/(1+np.exp(X @ w-b*np.ones(100)))\n    act = 1/(1+np.exp(X @ w-b*np.ones(100)))\n    grad_w = np.zeros(4)\n    for i in range(0,4):\n        grad_w[i] = np.sum(e * act * (1-act) * X[:,i])\n    grad_b = -np.sum(e *act * (1-act))\n    return np.concatenate([grad_w, np.array([grad_b])])*1/100\n\n# Steepest Descent\ndef steepest_descent(start, alpha=0.1,tol=1e-6, max_iter=150000):\n    theta = np.array(start, dtype=float)\n    iterates = [theta.copy()]\n    optimal_values=[L(theta[:4],theta[4])]\n    for _ in range(max_iter):\n        grad = grad_L(theta[:4], theta[4])\n        \n        # Newton step: x_new = x - H_inv * grad\n        theta -= alpha*grad\n        \n        iterates.append(theta.copy())\n        optimal_values.append(L(theta[:4],theta[4]))\n        \n        # Convergence check\n        if np.linalg.norm(grad) &lt; tol:\n            break\n    \n    return theta, iterates, optimal_values\n\n\n# Pick an initial point $\\theta_0$\nstart_point = np.array([1.0,1.0,-5.0,0.0, -10.0])*0.0\n\n# Run gradient descent \noptimum, iterates, optimal_values = steepest_descent(start_point)\n\n# Extract iterate points for printing and plotting\niterates = np.array(iterates)\nprint(f\"w = {iterates[-1,:4]}, b = {iterates[-1,4]}\")\n\n# Plotting: Loss\nplt.figure(figsize=(8, 6))\nplt.plot(range(0,len(optimal_values)),optimal_values,  'o-', color=\"blue\")\nplt.xlabel(\"iterates (k)\")\nplt.ylabel(f\"Loss\")\nplt.show()\n\n\n# Show Confusion Matrix\nw = iterates[-1,:4] \nb = iterates[-1,4]\ny_pred = 1/(1+np.exp(X @ w -b*np.ones(100)))\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y,np.round(y_pred))\nprint(\"Confusion Matrix:\\n\",cm)",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Models"
    ]
  },
  {
    "objectID": "simplemodels.html#whats-next",
    "href": "simplemodels.html#whats-next",
    "title": "Simple Models",
    "section": "What’s Next",
    "text": "What’s Next\nEverything looks reasonable so far. We had data, we proposed parametrized models, and we fit the model parameters to the data using optimization and we looked at the results. We ignored the many challenges when working on real problems:\n\nWhat should the initial guess be for gradient descent? What should the step-size be?\nHow do you evaluate models when you can’t visually inspect the models outputs and ground truth?\nHow will you obtain gradients for more complex models?\nWhat happens to gradient descent when you have millions of data points?",
    "crumbs": [
      "Home",
      "Basics",
      "Simple Models"
    ]
  },
  {
    "objectID": "mlps.html",
    "href": "mlps.html",
    "title": "Multi-Layer Perceptrons",
    "section": "",
    "text": "The Perceptron is the composition of a linear function with an element-wise application of a nonlinear activation function \\(\\sigma\\):\n\\[\\begin{align}\n  y(x,\\theta) = \\sigma(f(x,\\theta)) = \\sigma(w^\\top x + b) = \\sigma\\left( \\sum w_i x_i + b\\right),\n\\end{align}\\] where \\(w\\) has the same length as \\(x\\).\nIt defines the simplest neural network: one neuron. The neuron receives inputs \\(x\\), linearly combines them into a scalar using weights \\(w\\) with a bias \\(b\\), and finally applies a nonlinearity \\(\\sigma(\\cdot)\\) to produce output \\(y\\).\n\n\nHow many layers does a perceptron have, given that it is a neural network? I’d say it has one layer, defined by the single neuron. Since the output of that neuron is the output of the network, it could be called the output layer. Is there an input layer? Often, the input is said to be generated by a set of input neurons in the input layer. So is the perceptron really a single neuron? The simple principle is that we only count neurons performing computations as being part of layers. The input neurons are just sources without computations, so everything works out.\n\n\n\nWhen the activation function is the sigmoid function and the model predicts a scalar value, fitting a perceptron model to the data is the same as solving a logistic regression problem. Once data are loaded, we can run nearly the same code with the exception of adding the activation function to the model.\nThe linear model\n\nclass Lin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net =  nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.net(x)\nmodel = Lin()\n\nbecomes\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(4, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\nmodel = MLP()\n\n\n\nInitial w:  tensor([[0., 0., 0., 0.]]) b: tensor([0.])\nStart training: \nEpoch 1/150000 - Loss: 0.2500\nEpoch 15001/150000 - Loss: 0.0291\nEpoch 30001/150000 - Loss: 0.0260\nEpoch 45001/150000 - Loss: 0.0245\nEpoch 60001/150000 - Loss: 0.0236\nEpoch 75001/150000 - Loss: 0.0230\nEpoch 90001/150000 - Loss: 0.0225\nEpoch 105001/150000 - Loss: 0.0221\nEpoch 120001/150000 - Loss: 0.0218\nEpoch 135001/150000 - Loss: 0.0216\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n  versicolor       0.98      0.96      0.97        50\n   virginica       0.96      0.98      0.97        50\n\n    accuracy                           0.97       100\n   macro avg       0.97      0.97      0.97       100\nweighted avg       0.97      0.97      0.97       100",
    "crumbs": [
      "Home",
      "Basics",
      "Multi-Layer Perceptrons"
    ]
  },
  {
    "objectID": "mlps.html#the-perceptron",
    "href": "mlps.html#the-perceptron",
    "title": "Multi-Layer Perceptrons",
    "section": "",
    "text": "The Perceptron is the composition of a linear function with an element-wise application of a nonlinear activation function \\(\\sigma\\):\n\\[\\begin{align}\n  y(x,\\theta) = \\sigma(f(x,\\theta)) = \\sigma(w^\\top x + b) = \\sigma\\left( \\sum w_i x_i + b\\right),\n\\end{align}\\] where \\(w\\) has the same length as \\(x\\).\nIt defines the simplest neural network: one neuron. The neuron receives inputs \\(x\\), linearly combines them into a scalar using weights \\(w\\) with a bias \\(b\\), and finally applies a nonlinearity \\(\\sigma(\\cdot)\\) to produce output \\(y\\).\n\n\nHow many layers does a perceptron have, given that it is a neural network? I’d say it has one layer, defined by the single neuron. Since the output of that neuron is the output of the network, it could be called the output layer. Is there an input layer? Often, the input is said to be generated by a set of input neurons in the input layer. So is the perceptron really a single neuron? The simple principle is that we only count neurons performing computations as being part of layers. The input neurons are just sources without computations, so everything works out.\n\n\n\nWhen the activation function is the sigmoid function and the model predicts a scalar value, fitting a perceptron model to the data is the same as solving a logistic regression problem. Once data are loaded, we can run nearly the same code with the exception of adding the activation function to the model.\nThe linear model\n\nclass Lin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net =  nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.net(x)\nmodel = Lin()\n\nbecomes\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(4, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return self.net(x)\nmodel = MLP()\n\n\n\nInitial w:  tensor([[0., 0., 0., 0.]]) b: tensor([0.])\nStart training: \nEpoch 1/150000 - Loss: 0.2500\nEpoch 15001/150000 - Loss: 0.0291\nEpoch 30001/150000 - Loss: 0.0260\nEpoch 45001/150000 - Loss: 0.0245\nEpoch 60001/150000 - Loss: 0.0236\nEpoch 75001/150000 - Loss: 0.0230\nEpoch 90001/150000 - Loss: 0.0225\nEpoch 105001/150000 - Loss: 0.0221\nEpoch 120001/150000 - Loss: 0.0218\nEpoch 135001/150000 - Loss: 0.0216\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n  versicolor       0.98      0.96      0.97        50\n   virginica       0.96      0.98      0.97        50\n\n    accuracy                           0.97       100\n   macro avg       0.97      0.97      0.97       100\nweighted avg       0.97      0.97      0.97       100",
    "crumbs": [
      "Home",
      "Basics",
      "Multi-Layer Perceptrons"
    ]
  },
  {
    "objectID": "mlps.html#multi-layer-perceptron",
    "href": "mlps.html#multi-layer-perceptron",
    "title": "Multi-Layer Perceptrons",
    "section": "Multi-Layer Perceptron",
    "text": "Multi-Layer Perceptron\nThe output of one set of perceptrons can be treated as the input to another set of perceptrons. Each set is called a layer of a multi-layer perceptron. When you use non-sigmoid activations, you get a generic artificial feed-forward neural network (AFFNN). Nowadays people call an AFFNN an MLP regardless of the activation function.\nWhen we have \\(L\\) layers, the neural network function may be written as\n\\[\\begin{align}\n  y = x_L = \\sigma\\ \\circ \\mathrm{Aff}_L \\circ \\sigma\\ \\circ \\mathrm{Aff}_{L-1} \\circ \\sigma\\ \\circ \\mathrm{Aff}_{L-2}  \\circ \\cdots \\circ \\sigma\\ \\circ \\mathrm{Aff}_{2} \\circ \\sigma\\ \\circ \\mathrm{Aff}_{1} (x_0),\n\\end{align}\\] where \\(\\mathrm{Aff}_i(x) = W_i x + b_i\\).\nAn example code snippet of an MLP defined using torch.nn:\n\nself.net = nn.Sequential(\n    nn.Linear(4, 64),\n    nn.Sigmoid(),\n    nn.Linear(64, 32),\n    nn.Sigmoid(),\n)\n\nThe code says that this network has two layers, with the first layer having \\(64\\) neurons and the second layer having \\(32\\). So the output dimension is \\(y \\in \\mathbb{R}^{32}\\). The activations are sigmoids. We also expect the network to have parameters \\(W_1 \\in \\mathbb{R}^{64 \\times 4}\\), \\(b_1 \\in \\mathbb{R}^{64}\\), \\(W_2 \\in \\mathbb{R}^{32 \\times 64}\\) and \\(b_2 \\in \\mathbb{R}^{32}\\).\n\n\nname: net.0.weight   size: torch.Size([64, 4])\nname: net.0.bias     size: torch.Size([64])\nname: net.2.weight   size: torch.Size([32, 64])\nname: net.2.bias     size: torch.Size([32])",
    "crumbs": [
      "Home",
      "Basics",
      "Multi-Layer Perceptrons"
    ]
  },
  {
    "objectID": "mlps.html#application-iris-classification",
    "href": "mlps.html#application-iris-classification",
    "title": "Multi-Layer Perceptrons",
    "section": "Application: Iris Classification",
    "text": "Application: Iris Classification\nWe modify the code used in the lesson on PyTorch to build a full classifier for the iris dataset. There are some differences from that code and from the two-class case:\n\nWe transform the data to normalize it, meaning its mean is zero and ‘variance’ is near \\(1\\).\nWe split data into train and test sets.\nInstead of a model that outputs a single scalar (fine for two species), we now predict three numbers, one for each species. Whichever value is highest for an input determines its species.\nWe use a cross-entropy loss instead of a mean squared error loss, which measures how far away we are from predicting a value of \\(1\\) for the true species and \\(0\\) for the others. It wants the model to behave like a probability distribution over the species. This desire is often made explicit using a softmax final layer.\nWe use an in-built optimizer instead of rolling our own gradient update.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\n\n# 1. Load and preprocess data\niris = load_iris()\nX = iris.data\ny = iris.target\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n# 2. Define MLP model\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(4, 64),\n            nn.Sigmoid(),\n            nn.Linear(64, 32),\n            nn.Sigmoid(),\n            nn.Linear(32, 3),  # 3 classes\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nmodel = MLP()\n\n# 3. Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# 4. Training loop\nfor epoch in range(100):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/100 - Loss: {loss.item():.4f}\")\n\n# 5. Evaluation\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(X_test)\n    _, predicted = torch.max(outputs, 1)\n    print(\"\\nClassification Report:\\n\")\n    print(classification_report(y_test.numpy(), predicted.numpy(), target_names=iris.target_names))\n\nEpoch 10/100 - Loss: 0.8600\nEpoch 20/100 - Loss: 0.4502\nEpoch 30/100 - Loss: 0.2779\nEpoch 40/100 - Loss: 0.1800\nEpoch 50/100 - Loss: 0.1144\nEpoch 60/100 - Loss: 0.0759\nEpoch 70/100 - Loss: 0.0603\nEpoch 80/100 - Loss: 0.0541\nEpoch 90/100 - Loss: 0.0497\nEpoch 100/100 - Loss: 0.0471\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n      setosa       1.00      1.00      1.00        10\n  versicolor       1.00      1.00      1.00        10\n   virginica       1.00      1.00      1.00        10\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30",
    "crumbs": [
      "Home",
      "Basics",
      "Multi-Layer Perceptrons"
    ]
  },
  {
    "objectID": "mnist.html#classifying-mnist",
    "href": "mnist.html#classifying-mnist",
    "title": "Deep Learning Course",
    "section": "Classifying MNIST",
    "text": "Classifying MNIST\n\n\nDL Course\n\n\n\n\n\n\n\nInstructor: Hasan A. Poonawala, Joseph Kershaw  Mechanical and Aerospace Engineering  University of Kentucky, Lexington, KY, USA",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-dataset",
    "href": "mnist.html#mnist-dataset",
    "title": "Deep Learning Course",
    "section": "MNIST Dataset",
    "text": "MNIST Dataset\n\nGoal: Zip-code recognition for mail sorting\n60,00060,000 training images of digits 00-99\n10,00010,000 test images\nSmaller USPS dataset had 72917291 train and 20072007 test images\n\nHumans had a 2.5% error rate (source).",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-samples",
    "href": "mnist.html#mnist-samples",
    "title": "Deep Learning Course",
    "section": "MNIST Samples",
    "text": "MNIST Samples\n\nEach image is 28×2828 \\times 28 with grayscale value in [0,1][0,1]",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-samples-1",
    "href": "mnist.html#mnist-samples-1",
    "title": "Deep Learning Course",
    "section": "MNIST Samples",
    "text": "MNIST Samples\nimport torch ## See `./code/mnist/viz_mnist.py`\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 1. Data\ntransform = transforms.Compose([ ## transform the raw data from the file while loading\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)) ## mean, std image -&gt; (image - mean)/std\n])\n\ntrain_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n\n\nrandom_array = np.random.randint(0, 59999, size=6)\n\nfig, axes = plt.subplots(nrows=2, ncols=3, figsize=(10, 8))\naxes = axes.flatten()\n\nfor i in range(6):\n    image, label = train_dataset[random_array[i]]\n    axes[i].imshow(image[0], cmap=\"gray\")\n    axes[i].set_title(f\"label: {label}\")",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-classifier",
    "href": "mnist.html#mnist-classifier",
    "title": "Deep Learning Course",
    "section": "MNIST Classifier",
    "text": "MNIST Classifier\n\n\n\n\nTest set: Accuracy: 9841/10000 (98%)",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-classifier-1",
    "href": "mnist.html#mnist-classifier-1",
    "title": "Deep Learning Course",
    "section": "MNIST Classifier",
    "text": "MNIST Classifier\n## See `./code/mnist/test_mnist.py`\nimport torch\nfrom torchvision.datasets import MNIST\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\n\n# 1. Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x.view(-1))\n])\n\ntrain_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\ntest_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\ntest_plot_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n\n\n\n# 2. Model\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(784, 512),\n            nn.Sigmoid(),\n            nn.Linear(512, 256),\n            nn.Sigmoid(),\n            nn.Linear(256, 128),\n            nn.Sigmoid(),\n            nn.Linear(128, 64),\n            nn.Sigmoid(),\n            nn.Linear(64, 10),  # 10 classes\n        )\n\n    def forward(self, x):\n        return F.log_softmax(self.net(x), dim=1)\n\nmodel = Net()\n\nPATH = './mnist_model.pth'\nmodel.load_state_dict(torch.load(PATH))\n\n# x. Evaluate\ncorrect = 0\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(16, 8))\naxes = axes.flatten()\n\nwith torch.no_grad():\n    for data, target in test_loader:\n        output = model(data)\n        pred = output.argmax(dim=1, keepdim=True)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n    for i in range(0,10):\n        images, labels = next(iter(test_plot_loader))\n        output = model(images)\n        pred = output.argmax(dim=1, keepdim=True)\n        img, lbl = images[0].squeeze(), labels.item()\n        axes[i].imshow(img.view(28,28), cmap=\"gray\")\n        axes[i].set_title(f\"true: {lbl} predicted: {pred.item()}\")\n    plt.show()\n\nprint('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    correct, len(test_loader.dataset),\n    100. * correct / len(test_loader.dataset)))",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-training",
    "href": "mnist.html#mnist-training",
    "title": "Deep Learning Course",
    "section": "MNIST Training",
    "text": "MNIST Training",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#mnist-training-1",
    "href": "mnist.html#mnist-training-1",
    "title": "Deep Learning Course",
    "section": "MNIST Training",
    "text": "MNIST Training\n## See `./code/mnist/mnist_mlp.py`\n# 1. Data \n# 2. Model\n# 3. Loss and 4. Training\n## Skips the plotting portion. See `mnist_mlp.py`\noptimizer = optim.Adam(model.parameters())\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        ## Negative Log Likelihood loss defined within training loop:\n        loss = F.nll_loss(output, target) \n        loss.backward() # obtain gradients\n        optimizer.step() # update parameters \"descent step\"\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n# 5. Evaluate\n\n\n# 6. Save model\nPATH = './mnist_model.pth'\ntorch.save(model.state_dict(), PATH)",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "mnist.html#cnns-and-mnist",
    "href": "mnist.html#cnns-and-mnist",
    "title": "Deep Learning Course",
    "section": "CNNs and MNIST",
    "text": "CNNs and MNIST\n## See `./code/mnist/mnist_cnn.py`\n# 1. Data \n# 2. Model\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(7 * 7 * 64, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 7 * 7 * 64)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\nmodel = Net()\n\n# 3. Loss and 4. Training\n# 5. Evaluate\n# 6. Save model",
    "crumbs": [
      "Home",
      "Advanced",
      "Classifying MNIST"
    ]
  },
  {
    "objectID": "CNNs.html",
    "href": "CNNs.html",
    "title": "CNNs",
    "section": "",
    "text": "Outline: Using Pytorch for CNNs and Kernel Math",
    "crumbs": [
      "Home",
      "Advanced",
      "CNNs"
    ]
  },
  {
    "objectID": "CNNs.html#convolution-layers-in-pytorch",
    "href": "CNNs.html#convolution-layers-in-pytorch",
    "title": "CNNs",
    "section": "Convolution Layers in Pytorch",
    "text": "Convolution Layers in Pytorch\nUnlike linear layers, Convolution layers have more parameters required to initialized. The first two arguments are the `in_channels’ & ‘out_channels’. From a data perspective, a Channel is some stream of data, so if you are looking at one time series of sound data, you have 1 channel. Conversely, a color image is a 3 channel input for a 2D convolution.\nThis is analogous to the in_neurons and out_neurons of a linear layer, and they must be maintained throughout your network, e.g., a Convolution Layer with an ‘out_channels’ of 3 must be fed into a Convolution Layer with an ‘in_channels’ of 3.\nThe next input is the kernel size. This is how much that layer “sees” at any given time. For higher dimensional kernels, you can have different sizes across your dimensions, but normally they are all the same.\nNext is stride, which is how “fast” a kernel moves across the dataset, a longer stride is analogous to a passing glance, whereas a shorter one is like an in depth search.\nThe final main parameter is padding, which is how much non-impactful data (usually zeros) are added to the edge of your input. This is used for 2 reasons, the first is that you want to emphasize the edges of your data more. Secondly, it allows you to adjust the number of features leaving a convolution layer.\nA nice animation that shows these is found here.\n\nimport torch\nimport torch.nn as nn\n\nbatches:int = 2\nchannels:int = 3\nlength_1d:int = 50\nheight_2d:int = 40\nwidth_2d:int = 40\n\ninput_1d = torch.randn(batches,channels,length_1d)\ninput_2d = torch.randn(batches,channels,height_2d,width_2d)\n\nchannels_out:int = 5\nkernel_size:int = 3\nstride:int = 5\npadding:int = 3\nconv1d = nn.Conv1d(channels,channels_out,kernel_size,stride,padding)\nconv2d = nn.Conv2d(channels,channels_out,kernel_size,stride,padding)\n\nprint('Size of 1D Output:')\nprint(conv1d(input_1d).size())\n\nprint('Size of 2D Output:')\nprint(conv2d(input_2d).size())\n\nSize of 1D Output:\ntorch.Size([2, 5, 11])\nSize of 2D Output:\ntorch.Size([2, 5, 9, 9])\n\n\nAs you can see, the channels are hard coded into the linear layer, but the size of the data dimensions have changed. This is where convolution arithmetic comes into play.\n\\(L~out~ = \\lfloor\\frac{L~in~ + 2 \\times padding - size}{stride}\\rfloor + 1\\)\nWhile this changing size doesn’t matter from a network perspective (as long as you are only using convolution layers), it has 2 main implications. The first is that you don’t want to completely lose your data down to nothing. The second is that you usually want to convert to a linear layer, and you will need to know how many features are being fed into it for proper construction. While one way to handle this is to calculate the size for every layer, the other is to use the equation to force a known behavior for example, a kernel size of 3, a pad of 1, and a stride of 2, will always half the length of your input, making the math easy.",
    "crumbs": [
      "Home",
      "Advanced",
      "CNNs"
    ]
  },
  {
    "objectID": "RealData.html",
    "href": "RealData.html",
    "title": "Real Data",
    "section": "",
    "text": "Outline: Using a Real Dataset",
    "crumbs": [
      "Home",
      "Advanced",
      "Real Data"
    ]
  },
  {
    "objectID": "RealData.html#analyze-data",
    "href": "RealData.html#analyze-data",
    "title": "Real Data",
    "section": "Analyze Data",
    "text": "Analyze Data\nLets see how we can quickly train a new model on a new dataset by using a DataSet. Let’s first look at our new Data, this example is Welding_Perceptron_data.xlsx.\n\nimport pandas as pd\n\ndf = pd.read_excel(r\"_dlcourse/code/Linear Models/Welding_Perceptron_data.xlsx\") #You could also just open this file in Excel \n\npd.set_option('display.colheader_justify', 'center') #pretty print option\npd.set_option('display.precision',3) #pretty print option\n\nprint(f\"Total number of samples: {len(df)}\")\nprint(\"Top 5 rows:\")\nprint(df.head())\n\nTotal number of samples: 480\nTop 5 rows:\n   TrialNo  Current  Angle  Speed  Time   Height\n0     1       160      0    0.005  5.003    39  \n1     1       160      0    0.005  5.502    39  \n2     1       160      0    0.005  5.969    39  \n3     1       160      0    0.005  6.453    38  \n4     1       160      0    0.005  6.920    40  \n\n\nAs you can see, we have 6 columns of data. For this experiment, I want to coorelate the Current, Angle, Speed, and Time to the Height.",
    "crumbs": [
      "Home",
      "Advanced",
      "Real Data"
    ]
  },
  {
    "objectID": "RealData.html#create-a-dataset",
    "href": "RealData.html#create-a-dataset",
    "title": "Real Data",
    "section": "Create a Dataset",
    "text": "Create a Dataset\nLet’s first meet the requirments for creating a custom pytorch Dataset, there are three requirements: Inhereit the Dataset class, create a len function, and create a getitem function. Lets also give it a name.\n\nfrom torch.utils.data import Dataset\n\nclass WeldingRegression(Dataset):\n    def __init__(self):\n        super().__init__() \n        \n    \n    def __len__(self):\n      \n    \n    def __getitem__(self, index):\n\nNext, let’s figure out how we are going to read the relevant data in python.\n\nimport pandas as pd\nfrom torch.utils.data import Dataset\n\nclass WeldingRegression(Dataset):\n    def __init__(self):\n        super().__init__()\n\n        df_data = pd.read_excel(\"Welding_Perceptron_data.xlsx\") \n        input_names =  ['Current','Angle','Speed','Time']\n        target_names = ['Height']\n        df_inputs = df_data[input_names]\n        df_targets = df_data[target_names]\n\nNow that we have the relevant data separated out, we can convert them to tensors\n\nimport pandas as pd\nimport torch \nfrom torch.utils.data import Dataset\n\nclass WeldingRegression(Dataset):\n    def __init__(self):\n        super().__init__()\n\n        df_data = pd.read_excel(\"Welding_Perceptron_data.xlsx\")\n        input_names =  ['Current','Angle','Speed','Time']\n        target_names = ['Height']\n        df_inputs = df_data[input_names]\n        df_targets = df_data[target_names]\n\n        ar_inputs = df_inputs.values\n        ar_targets = df_targets.values\n        self.tensor_inputs = torch.tensor(ar_inputs).float()\n        self.tensor_targets = torch.tensor(ar_targets).float()\n\nWe have now prepared the data for our data loader, the final thing is to include the len function needs to return the total number of samples (that will be going across axis 0). and the getitem\n\ndef __len__(self):\n    return len(self._tensor_targets)\n\ndef __getitem__(self, index):\n    inputs = self._tensor_inputs[index]\n    targets = self._tensor_targets[index]\n    return inputs,targets\n\nFinally, we want the ability to normalize our data, so we create a normalize function, and apply it to our tensors.\n\nif normalize:\n    self.normalize(self._tensor_inputs)\n    self.normalize(self._tensor_targets)\n\ndef normalize(self,tensor:torch.Tensor) -&gt; None:\n      tensor[:] = (tensor-tensor.amin(0))/(tensor.amax(0)-tensor.amin(0))",
    "crumbs": [
      "Home",
      "Advanced",
      "Real Data"
    ]
  },
  {
    "objectID": "RealData.html#clean-up",
    "href": "RealData.html#clean-up",
    "title": "Real Data",
    "section": "Clean Up",
    "text": "Clean Up\n\ndef normalize_tensor(tensor:torch.Tensor) -&gt; None:\n    tensor[:] = (tensor-tensor.amin(0))/(tensor.amax(0)-tensor.amin(0))\n\nclass WeldingDataset(Dataset):\n    def __init__(self,\n                 data_file:str,\n                 input_names:List,\n                 target_names:List,\n                 normalize:bool=False,\n                 ):\n        super().__init__()\n\n        df_data:pd.DataFrame = pd.read_excel(data_file) \n        df_inputs = df_data[input_names]\n        df_targets = df_data[target_names]\n        self._tensor_inputs = torch.tensor(df_inputs.values).float()\n        self._tensor_targets = torch.tensor(df_targets.values).view(-1,len(target_names)).float()\n        if normalize:\n            normalize_tensor(self._tensor_inputs)\n            normalize_tensor(self._tensor_targets)\n        \n    \n    def __len__(self):\n        return len(self._tensor_targets)\n    \n    def __getitem__(self, index):\n        inputs = self._tensor_inputs[index]\n        targets = self._tensor_targets[index]\n        return inputs,targets",
    "crumbs": [
      "Home",
      "Advanced",
      "Real Data"
    ]
  },
  {
    "objectID": "RealData.html#use-new-data",
    "href": "RealData.html#use-new-data",
    "title": "Real Data",
    "section": "Use new Data",
    "text": "Use new Data\n\nif __name__ == \"__main__\":\n    seed_everything(1)\n    input_names =  ['Current','Angle','Speed','Time']\n    target_names = ['Height']\n    dataset = WeldingDataset(\"Welding_Perceptron_data.xlsx\",input_names,target_names,False)\n    ...\n    model = mlpModel(len(input_names),10,len(target_names))",
    "crumbs": [
      "Home",
      "Advanced",
      "Real Data"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains material for a crash course on working with deep models.\nInstructors: Hasan A. Poonawala, Joseph Kershaw."
  },
  {
    "objectID": "pytorch.html",
    "href": "pytorch.html",
    "title": "PyTorch",
    "section": "",
    "text": "PyTorch, accessed via the torch package, is a scientific computing framework centered on representing data as tensors and automatic differentiation of tensor operations, making it easy to implement differentiable computing. Deep learning is one of its most popular applications, where differentiation of loss functions to form gradients is key to training models on data.",
    "crumbs": [
      "Home",
      "Basics",
      "PyTorch"
    ]
  },
  {
    "objectID": "pytorch.html#introduction",
    "href": "pytorch.html#introduction",
    "title": "PyTorch",
    "section": "",
    "text": "PyTorch, accessed via the torch package, is a scientific computing framework centered on representing data as tensors and automatic differentiation of tensor operations, making it easy to implement differentiable computing. Deep learning is one of its most popular applications, where differentiation of loss functions to form gradients is key to training models on data.",
    "crumbs": [
      "Home",
      "Basics",
      "PyTorch"
    ]
  },
  {
    "objectID": "pytorch.html#tensors",
    "href": "pytorch.html#tensors",
    "title": "PyTorch",
    "section": "Tensors",
    "text": "Tensors\nTensors in torch are \\(n\\)-dimensional array objects, similar to numpy arrays. In fact, you can easily switch:\n\nimport numpy as np\nimport torch\n\na = np.array([1.0, 2.0])\nb = torch.from_numpy(a)           # NumPy → PyTorch (shares memory!)\nc = b.numpy()                     # PyTorch → NumPy (also shares memory!)\n\nHowever, torch tensor objects contain additional information and methods related to automatic differentiation and where it is located in system memory.",
    "crumbs": [
      "Home",
      "Basics",
      "PyTorch"
    ]
  },
  {
    "objectID": "pytorch.html#autograd",
    "href": "pytorch.html#autograd",
    "title": "PyTorch",
    "section": "Autograd",
    "text": "Autograd\nDeep learning is nearly impossible without the ability to automatically differentiate models. Before that, demonstrating the value of a new model required careful derivation and implementation of its gradients. Now, all of that work has been automated away.\n\nimport torch\n\nt = torch.Tensor([0.8762])\nprint(t) # This variable is not considered for gradient computations\nt.requires_grad = True \nprint(t) # Now it is\ntorch.sigmoid(t).backward() # calculate gradient (of sigmoid(t))\nmanual_grad = torch.sigmoid(t) * (1 - torch.sigmoid(t)) # manually calculated gradient of sigmoid\nprint(\"torch grad: \", t.grad.item(),\" manual gradient: \", manual_grad.item())\n\ntensor([0.8762])\ntensor([0.8762], requires_grad=True)\ntorch grad:  0.20754991471767426  manual gradient:  0.20754991471767426\n\n\nNote that \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\) has derivative \\(\\sigma'(x) = \\frac{e^{-x}}{(1+e^{-x})^2} = \\sigma(x) (1-\\sigma(x))\\)",
    "crumbs": [
      "Home",
      "Basics",
      "PyTorch"
    ]
  },
  {
    "objectID": "pytorch.html#application-linear-model",
    "href": "pytorch.html#application-linear-model",
    "title": "PyTorch",
    "section": "Application: Linear Model",
    "text": "Application: Linear Model\nWe fit a linear model to scalar input-output data in the simple models lesson using gradient descent ‘by hand’. Here, we use PyTorch. There are two differences from the pattern we saw at the end of the lesson on simple models:\n\nThe model is defined separately from the loss.\nWe don’t implement a gradient function, autodiff will derive it for us.\n\n\n## Plot iterates and loss under GD using torch\n## Train a linear model using pytorch\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 1. Create data\n## First as numpy array\nx = np.linspace(-1,1,100) \ny_noise_free = 4*x + 5\ny = y_noise_free+0.5*np.random.randn(100)\n\n\n## Create tensors, as 2D arrays using unsqueeze \nX_train = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\ny_train = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n\nprint(X_train.shape) ## two dimensions now\nprint(y_train.shape)\nprint(X_train.requires_grad)\n\n\n# 2. Define Linear model\nclass Lin(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net =  nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.net(x)\nmodel = Lin()\n\n# 3. Define loss \ncriterion = nn.MSELoss() # Mean Squared Error\n\nprint(\"Start training: \")\n# 4. Training loop\nfor epoch in range(5000):\n    model.train()       # Tell pytorch that model computations are for training now\n    model.zero_grad()   # clear accumulated grads\n    outputs = model(X_train) # predict values ... \n    loss = criterion(outputs, y_train) # ... so we can compute loss \n    loss.backward()     # Automatically get gradients of loss wrt model as it is right now\n    with torch.no_grad(): ## Manually implementing gradient descent. Don't want the update to be tracked by autograd\n        for param in model.parameters():\n            param -= 0.005 * param.grad  # θ ← θ − α∇θ L\n\n    if (epoch + 1) % 500 == 0: ## Show us the loss as training proceeds\n        print(f\"Epoch {epoch+1}/5000 - Loss: {loss.item():.4f}\")\n\n# 5. Evaluation\nmodel.eval()    # tell pytorch that we will use the model in evaluation mode\nwith torch.no_grad():\n    outputs = model(X_train).numpy() # convert predictions to numpy array to work with matplotlib\n    plt.scatter(x,y,label=\"noise data\")\n    plt.scatter(x,y_noise_free,label=\"true (noise-free)\")\n    plt.scatter(x,outputs[:,0],label=\"predicted\")\n    plt.legend()\n\ntorch.Size([100, 1])\ntorch.Size([100, 1])\nFalse\nStart training: \nEpoch 500/5000 - Loss: 0.4987\nEpoch 1000/5000 - Loss: 0.2504\nEpoch 1500/5000 - Loss: 0.2422\nEpoch 2000/5000 - Loss: 0.2420\nEpoch 2500/5000 - Loss: 0.2419\nEpoch 3000/5000 - Loss: 0.2419\nEpoch 3500/5000 - Loss: 0.2419\nEpoch 4000/5000 - Loss: 0.2419\nEpoch 4500/5000 - Loss: 0.2419\nEpoch 5000/5000 - Loss: 0.2419",
    "crumbs": [
      "Home",
      "Basics",
      "PyTorch"
    ]
  },
  {
    "objectID": "leresnet.html#model-re-use",
    "href": "leresnet.html#model-re-use",
    "title": "Deep Learning Course",
    "section": "Model Re-use",
    "text": "Model Re-use\n\n\nDL Course\n\n\n\n\n\n\n\nInstructor: Hasan A. Poonawala, Joseph Kershaw  Mechanical and Aerospace Engineering  University of Kentucky, Lexington, KY, USA",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#overview",
    "href": "leresnet.html#overview",
    "title": "Deep Learning Course",
    "section": "Overview",
    "text": "Overview\n\nBuild LeNet\nFinetune or Train ResNet50 on CIFAR10\n\nResnet50 was trained on ImageNet (1000 classes)\nCan you retrain it for the smaller CIFAR10 dataset?\n10 classes, 6000 images each: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#lenet",
    "href": "leresnet.html#lenet",
    "title": "Deep Learning Course",
    "section": "LeNet",
    "text": "LeNet\n\n\n\n\n\n\n\nA batch of input data has size N×1×28×28N \\times 1 \\times 28 \\times 28\nWhat is the size of the tensor after each block? Write them out.",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#lenet-foldermodels.py",
    "href": "leresnet.html#lenet-foldermodels.py",
    "title": "Deep Learning Course",
    "section": "LeNet <folder>/models.py",
    "text": "LeNet &lt;folder&gt;/models.py\nTask: implement LeNet in PyTorch\nimport torch\nimport torch.nn as nn\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n        ## Fill this in\n        )\n    \n    def forward(self, x):\n        return self.net(x)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#lenet-folderdata.py",
    "href": "leresnet.html#lenet-folderdata.py",
    "title": "Deep Learning Course",
    "section": "LeNet <folder>/data.py",
    "text": "LeNet &lt;folder&gt;/data.py\nimport torch\nfrom torchvision import datasets, transforms\n\ndef load_MNIST(batch_size=64):\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    test_dataset = datasets.MNIST('data', train=False, download=True, transform=transform)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, shuffle=False)\n    test_plot_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n    return train_dataset, train_loader, test_dataset, test_loader, test_plot_loader\n\ndef eval_MNIST(model,test_loader):\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#lenet-foldertest_lenet.py",
    "href": "leresnet.html#lenet-foldertest_lenet.py",
    "title": "Deep Learning Course",
    "section": "LeNet <folder>/test_leNet.py",
    "text": "LeNet &lt;folder&gt;/test_leNet.py\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom data import load_MNIST, eval_MNIST\nfrom models import LeNet\n\n\n\n_, _, test_dataset, test_loader, test_plot_loader = load_MNIST()\n\nmodel = LeNet()\n\n\neval_MNIST(model,test_loader)\n\nPATH = './mnist_model_cnn.pth'\nmodel.load_state_dict(torch.load(PATH))\n\neval_MNIST(model,test_loader)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#overview-1",
    "href": "leresnet.html#overview-1",
    "title": "Deep Learning Course",
    "section": "Overview",
    "text": "Overview\n\nBuild LeNet\nFinetune or Train ResNet50 on CIFAR10\n\nResnet50 was trained on ImageNet (1000 classes)\nCan you retrain it for the smaller CIFAR10 dataset?\n10 classes, 6000 images each: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks.",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#cifar10",
    "href": "leresnet.html#cifar10",
    "title": "Deep Learning Course",
    "section": "CIFAR10",
    "text": "CIFAR10",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#resnet50",
    "href": "leresnet.html#resnet50",
    "title": "Deep Learning Course",
    "section": "ResNet50",
    "text": "ResNet50",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#try-this",
    "href": "leresnet.html#try-this",
    "title": "Deep Learning Course",
    "section": "Try this",
    "text": "Try this\nDownload code/resnets/data.py and run this:\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport os\nfrom data import load_cifar, test_cifar, show_cifar\n\nif __name__ == \"__main__\":\n    #1. Load Data\n\n    transform = transforms.Compose(\n        [ transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    classes, trainset, trainloader, testset, testloader = load_cifar(transform,batch_size=16)\n\n    device = torch.device(\"cpu\")\n    print(f'Using {device} for inference')\n\n    #2. Load Model\n    ## Try this direct model\n    model = models.resnet50(pretrained=True)\n    model.to(device)\n\n    #5. Evaluate\n    print('Accuracy before training')\n    test_cifar(model,testloader,device,classes)\n    show_cifar(model,testloader,device,classes)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#customization",
    "href": "leresnet.html#customization",
    "title": "Deep Learning Course",
    "section": "Customization",
    "text": "Customization\ncode/resnets/models.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass ResNetCIFAR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet50 = models.resnet50(pretrained=True)\n        resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 10)\n        torch.nn.init.xavier_uniform_(resnet50.fc.weight)\n        self.resnet50 = resnet50\n\n    def forward(self,x):\n        return self.resnet50(x)\n\nclass ResNet18CIFAR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet18 = models.resnet18(pretrained=True)\n        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 10)\n        torch.nn.init.xavier_uniform_(resnet18.fc.weight)\n        self.resnet18 = resnet18\n\n    def forward(self,x):\n        return self.resnet18(x)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 100, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(100, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode):\n        super().__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        \n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)\n        return x\n    \n\nclass ResNetInt(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.de_layer3 = Interpolate(scale_factor=7, mode='bilinear')\n\n        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, 10)\n        nn.init.xavier_uniform_(self.resnet50.fc.weight)\n        self.decoder = nn.Sequential(\n            self.de_layer3,\n            self.resnet50\n        )\n    \n    def forward(self, x):\n        return self.decoder(x)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#finetune-resnet50",
    "href": "leresnet.html#finetune-resnet50",
    "title": "Deep Learning Course",
    "section": "Finetune Resnet50",
    "text": "Finetune Resnet50\ncode/resnets/finetune_resnet50_cifar10.py\n## Trains a model-mod where the final layer output size is reduced for CIFAR10, and CIFAR10 images are resized to 224x224. \n## We only train the fc layer\n## Based on https://pytorch.org/hub/nvidia_deeplearningexamples_model/\n## Modified by Hasan Poonawala \nimport torch\nfrom PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom data import load_cifar, test_cifar, show_cifar\nfrom utils import imshow\nfrom models import ResNetCIFAR\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nif __name__ == \"__main__\":\n\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256mb\"\n    torch.cuda.empty_cache()\n    #1. Load Data\n\n    transform = transforms.Compose(\n        [torchvision.transforms.Resize((224,224)), \n         transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    classes, trainset, trainloader, testset, testloader = load_cifar(transform,batch_size=16)\n\n\n    device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n    print(f'Using {device} for inference')\n\n    # PATH = 'cifar_model_v3.pth'\n    # model.load_state_dict(torch.load(PATH))\n    model = ResNetCIFAR()\n    model.to(device)\n\n    print('Accuracy before training')\n    test_cifar(model,testloader,device,classes)\n    show_cifar(model,testloader,device,classes)\n\n\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.resnet50.fc.parameters(), lr=0.0001, momentum=0.9)\n\n    print('Start training') \n    model.train()\n    for epoch in range(30):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            # inputs, labels = data\n            inputs, labels = data[0].to(device), data[1].to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 200 == 199:    # print every 2000 mini-batches\n                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n                running_loss = 0.0\n\n    print('Finished Training')\n\n    PATH = './cifar_model_v4.pth'\n    torch.save(model.state_dict(), PATH)\n\n\n\n    print('Accuracy after training')\n    test_cifar(model,testloader,device,classes)\n    show_cifar(model,testloader,device,classes)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#interpolate",
    "href": "leresnet.html#interpolate",
    "title": "Deep Learning Course",
    "section": "Interpolate",
    "text": "Interpolate\ncode/resnets/interpolate_resnet50_cifar10.py\n## Trains a resnet50-mod where the final layer output size is reduced for CIFAR10, and an interpolate layer accounts for input image size diff . As a result, batch size on homepc was 8, not 2 when using resize.\n## full net is trained\n## Based on https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/\n## Modified by Hasan Poonawala \nimport torch\nfrom PIL import Image\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport json\nimport requests\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfrom data import load_cifar, test_cifar, show_cifar\nfrom utils import imshow\nfrom models import ResNetInt\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nif __name__ == \"__main__\":\n\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:256mb\"\n    torch.cuda.empty_cache()\n    #1. Load Data\n\n    transform = transforms.Compose(\n        [ transforms.ToTensor(),\n         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    classes, trainset, trainloader, testset, testloader = load_cifar(transform,batch_size=16)\n\n\n    device = torch.device(\"mps\") if torch.mps.is_available() else torch.device(\"cpu\")\n    print(f'Using {device} for inference')\n\n    # PATH = 'cifar_model_v3.pth'\n    # model.load_state_dict(torch.load(PATH))\n    model = ResNetInt()\n    model.to(device)\n\n    print('Accuracy before training')\n    test_cifar(model,testloader,device,classes)\n    show_cifar(model,testloader,device,classes)\n\n\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.resnet50.fc.parameters(), lr=0.0001, momentum=0.9)\n\n    print('Start training') \n    model.train()\n    for epoch in range(10):  # loop over the dataset multiple times\n\n        running_loss = 0.0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            # inputs, labels = data\n            inputs, labels = data[0].to(device), data[1].to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            if i % 200 == 199:    # print every 2000 mini-batches\n                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n                running_loss = 0.0\n\n    print('Finished Training')\n\n    PATH = './cifar_model_v4.pth'\n    torch.save(model.state_dict(), PATH)\n\n\n\n    print('Accuracy after training')\n    test_cifar(model,testloader,device,classes)\n    show_cifar(model,testloader,device,classes)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#interpolate-1",
    "href": "leresnet.html#interpolate-1",
    "title": "Deep Learning Course",
    "section": "Interpolate",
    "text": "Interpolate\ncode/resnets/models.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass ResNetCIFAR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet50 = models.resnet50(pretrained=True)\n        resnet50.fc = torch.nn.Linear(resnet50.fc.in_features, 10)\n        torch.nn.init.xavier_uniform_(resnet50.fc.weight)\n        self.resnet50 = resnet50\n\n    def forward(self,x):\n        return self.resnet50(x)\n\nclass ResNet18CIFAR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        resnet18 = models.resnet18(pretrained=True)\n        resnet18.fc = torch.nn.Linear(resnet18.fc.in_features, 10)\n        torch.nn.init.xavier_uniform_(resnet18.fc.weight)\n        self.resnet18 = resnet18\n\n    def forward(self,x):\n        return self.resnet18(x)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 100, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(100, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1) # flatten all dimensions except batch\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode):\n        super().__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        \n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)\n        return x\n    \n\nclass ResNetInt(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.de_layer3 = Interpolate(scale_factor=7, mode='bilinear')\n\n        self.resnet50 = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_resnet50', pretrained=True)\n        self.resnet50.fc = nn.Linear(self.resnet50.fc.in_features, 10)\n        nn.init.xavier_uniform_(self.resnet50.fc.weight)\n        self.decoder = nn.Sequential(\n            self.de_layer3,\n            self.resnet50\n        )\n    \n    def forward(self, x):\n        return self.decoder(x)",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  },
  {
    "objectID": "leresnet.html#leresnet",
    "href": "leresnet.html#leresnet",
    "title": "Deep Learning Course",
    "section": "LeResNet",
    "text": "LeResNet\nFrom the paper",
    "crumbs": [
      "Home",
      "Advanced",
      "Model Re-use"
    ]
  }
]