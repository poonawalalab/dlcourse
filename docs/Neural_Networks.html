<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>neural_networks – Deep Learning Course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-3f13cc09417cf69baaf74afa0e2e3cf6.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Deep Learning Course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Basics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./simplemodels.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Simple Models</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pytorch.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">PyTorch</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multi-Layer Perceptrons</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Advanced</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RealData.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Real Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ResNet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Residual Neural Netowrk</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<p>
<span id="htoc" data-label="htoc"></span>
</p>
<h1 id="introduction">
Introduction
</h1>
<p>
Consider a model <span class="math inline"><em>ŷ</em> = <em>f</em>(<em>x</em>,<em>θ</em>)</span>
</p>
<p>
The loss of the model is <span class="math inline"><span class="math inline">\(L_{\theta}(x,y) =
\frac{1}{2} \lVert y -f(x,\theta) \rVert^2\)</span></span>.
</p>
<p>
The partial derivative of <span class="math inline"><em>L</em></span> with respect to <span class="math inline"><em>θ</em></span> is then <span class="math display"><span class="math display">\[\nabla_{\theta} L_{\theta}(x,y) =
-\underbrace{\underbrace{(y -f(x,\theta))^T}_{\text{co-vector}}
\underbrace{\nabla_{\theta}f(x,\theta)}_{\text{matrix}}
}_{\text{co-vector}}\]</span></span>
</p>
<p>
Now, the first row of <span class="math inline">∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>)</span> corresponds to <span class="math inline"><em>f</em><sub>1</sub>(<em>x</em>,<em>θ</em>)</span>, the second row to <span class="math inline"><em>f</em><sub>2</sub>(<em>x</em>,<em>θ</em>)</span>, and so on. The first <em>column</em> of <span class="math inline">∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>)</span> corresponds to <span class="math inline"><em>θ</em><sub>1</sub></span>, and so on again. Clearly, we’re assuming that <span class="math inline"><em>θ</em></span> is a vector, or a vectorized version of the parameters.
</p>
<p>
A gradient descent approach leads to parameter updates of the form <span class="math inline"><em>θ</em></span> (a vector) as <span class="math display"><em>θ</em><sub><em>k</em> + 1</sub> ← <em>θ</em><sub><em>k</em></sub> + <em>α</em>(∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>))<sup><em>T</em></sup>(<em>y</em>−<em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>)),</span> where <span class="math inline"><em>α</em></span> is a parameter that controls the step size of the update, and hence called a learning rate.
</p>
<p>
Note that gradient descent corresponds to <span class="math inline"><em>θ</em><sub><em>k</em> + 1</sub> ← <em>θ</em><sub><em>k</em></sub> + <em>v</em></span> where <span class="math inline"><em>v</em></span> is a vector that solves <span class="math display"><span class="math display">\[\begin{aligned}
  \min &amp;amp; \quad \langle \nabla_{\theta} L_{\theta}(x,y) , v\rangle \\
  \text{subject to}&amp;amp; \quad \lVert v \rVert_2 \leq \alpha \lVert
\nabla_{\theta} L_{\theta}(x,y) \rVert_2
\end{aligned}\]</span></span>
</p>
<h1 id="simple-models">
Simple Models
</h1>
<h2 id="linear-models" class="anchored">
Linear Models
</h2>
<p>
We derive the gradient updates for fitting a linear model using quadratic loss function.
</p>
<h3 id="matrix-calculus" class="anchored">
Matrix Calculus
</h3>
<p>
Let our model be a linear model <span class="math display"><em>ŷ</em> = <em>f</em>(<em>x</em>,<em>θ</em>) = <em>W</em><em>x</em> + <em>b</em>,  ⟹ <em>δ</em> = <em>y</em> − <em>f</em>(<em>x</em>,<em>θ</em>)</span> We want to minimize the error <span class="math inline"><span class="math inline">\(L =
\frac{1}{2}\lVert \delta\rVert^2 =  \sum_i \frac{1}{2}
\delta_i^2\)</span></span>. The gradient is simply <span class="math display"><span class="math display">\[\begin{aligned}
\frac{\partial L}{\partial W} &amp;amp;= \frac{\partial L}{\partial
\delta}\frac{\partial \delta}{\partial W} =   \sum_i
\delta_i  \frac{\partial \delta_i}{\partial W}=   \sum_i
\delta_i  \frac{\partial (-f_i(x,\theta))}{\partial W}\\
&amp;amp;=- \sum_i \delta_i \frac{\partial (e_i^T W x)}{\partial W}
\end{aligned}\]</span></span>
</p>
<p>
Since <span class="math inline"><span class="math inline">\(\frac{\partial a^T X b}{\partial X}
= a b^T\)</span></span> (derivation below), we get <span class="math display"><span class="math display">\[\begin{aligned}
\frac{\partial L}{\partial W} &amp;amp;=  - \sum_i \delta_i e_i x^T=  -
\left(  \sum_i \delta_i e_i \right) x^T = \underbrace{- \delta x^T}_{n
\times n\text{ matrix}}
\end{aligned}\]</span></span> The matrix update for <span class="math inline"><em>W</em></span> and vector update for <span class="math inline"><em>b</em></span> under gradient descent becomes <span class="math display"><em>W</em><sub><em>k</em> + 1</sub> ← <em>W</em><sub><em>k</em></sub> + <em>η</em><em>δ</em><sub><em>k</em></sub>&nbsp;<em>x</em><sup><em>T</em></sup>,</span> <span class="math display"><em>b</em><sub><em>k</em> + 1</sub> ← <em>b</em><sub><em>k</em></sub> + <em>η</em><em>δ</em><sub><em>k</em></sub>,</span> where <span class="math inline"><em>δ</em><sub><em>k</em></sub> = <em>y</em> − <em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>)</span>, <span class="math inline"><em>η</em></span> is the learning rate parameter, and <span class="math inline"><em>k</em></span> refers to time, not an element index.
</p>
<h4 id="derivation." class="anchored">
Derivation.
</h4>
<p>
Note that <span class="math inline"><em>f</em>(<em>X</em>) = <em>a</em><sup><em>T</em></sup><em>X</em><em>b</em> = ∑<sub><em>i</em></sub>∑<sub><em>j</em></sub><em>a</em><sub><em>i</em></sub><em>x</em><sub><em>i</em><em>j</em></sub><em>b</em><sub><em>j</em></sub></span>. Clearly, <span class="math inline"><span class="math inline">\(\frac{\partial f} {\partial x_{ij}}
= a_i b_j\)</span></span>. Therefore, if we represent <span class="math inline"><span class="math inline">\(\frac{\partial f} {\partial X}\)</span></span> as a matrix, we get <span class="math display"><span class="math display">\[\begin{aligned}
\frac{\partial } {\partial X} (a^T X b)&amp;amp;= a b^T  
\end{aligned}\]</span></span>
</p>
<h3 id="vector-calculus" class="anchored">
Vector Calculus
</h3>
<p>
This section derives the same updates as in the previous one, using a vectorization of parameters, instead of matrix calculus. This section may be skipped.
</p>
<p>
Let our model be a linear model <span class="math display"><em>f</em>(<em>x</em>,<em>θ</em>) = <em>W</em><em>x</em> + <em>b</em>,</span> with <span class="math inline"><em>W</em> = {<em>w</em><sub><em>i</em><em>j</em></sub>}</span> and <span class="math inline"><em>b</em> = {<em>b</em><sub><em>i</em></sub>}</span>, and <span class="math inline"><em>θ</em></span> being a vectorized version of <span class="math inline"><em>W</em></span> and <span class="math inline"><em>b</em></span>: <span class="math display"><span class="math display">\[\theta^T = \begin{bmatrix} W^1 &amp;amp; W^2
&amp;amp;  \cdots &amp;amp; W^n &amp;amp; b^T \end{bmatrix},\]</span></span> <span class="math display"><span class="math display">\[= \begin{bmatrix} w_{11} &amp;amp; w_{12} &amp;amp;
\cdots &amp;amp; w_{n1}&amp;amp;  \cdots&amp;amp;w_{nn} &amp;amp; b_1 &amp;amp; \cdots &amp;amp;
b_n \end{bmatrix}\]</span></span> where transposing <span class="math inline"><em>θ</em></span> makes it easier to associate each column of <span class="math inline">∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>)</span> with the right parameter.
</p>
<p>
Let’s compute some partial derivatives. To start, notice that <span class="math display"><em>f</em><sub><em>i</em></sub>(<em>x</em>,<em>θ</em>) = <em>w</em><sub><em>i</em>1</sub><em>x</em><sub>1</sub> + <em>w</em><sub><em>i</em>2</sub><em>x</em><sub>2</sub> + … + <em>w</em><sub><em>i</em><em>n</em></sub><em>x</em><sub><em>n</em></sub> + <em>b</em><sub><em>i</em></sub>.</span> Therefore, <span class="math display"><span class="math display">\[\frac{\partial f_i}{\partial
w_{jk}}  =
\begin{cases}
x_k &amp;amp; \text{ if }i = j\\
0 &amp;amp; \text{otherwise},
\end{cases} \text{ and}\]</span></span> <span class="math display"><span class="math display">\[\frac{\partial f_i}{\partial
b_{j}}  =\begin{cases}
1 &amp;amp; \text{ if }i = j\\
0 &amp;amp; \text{otherwise}.
\end{cases}\]</span></span>
</p>
<p>
We can check that <span class="math inline">∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>)</span> would ‘look’ like two <span class="math inline"><em>n</em> × <em>n</em></span> block diagonal matrices side-by-side. The diagonal elements of the first block diagonal matrix are each given by the co-vector <span class="math inline"><span class="math inline">\(x^T =
\begin{bmatrix}x_1 &amp;amp; x_2 &amp;amp; \cdots &amp;amp; x_n
\end{bmatrix}\)</span></span> and the second block diagonal matrix is just the identity matrix: <span class="math display"><span class="math display">\[\nabla_\theta f(x,\theta)
= \begin{bmatrix}  \left. \begin{matrix} x^T &amp;amp; 0 &amp;amp; \cdots
&amp;amp;  0 \\ 0 &amp;amp; x^T &amp;amp; \cdots &amp;amp; 0 \\ \vdots  &amp;amp; \vdots
&amp;amp; \ddots &amp;amp; \vdots\\ 0&amp;amp; 0 &amp;amp; \cdots &amp;amp;
x^T\end{matrix}\right| &amp;amp; I_{n}\end{bmatrix}.\]</span></span> A more compact approach would be to observe that the partial derivative written above leads to <span class="math inline">∇<sub><em>W</em><sup><em>i</em></sup></sub><em>f</em>(<em>x</em>,<em>W</em><sup><em>i</em></sup>) = <em>x</em><sup><em>T</em></sup></span>, and <span class="math inline">∇<sub><em>W</em><sup><em>j</em></sup></sub><em>f</em>(<em>x</em>,<em>W</em><sup><em>i</em></sup>) = 0</span> for <span class="math inline"><em>i</em> ≠ <em>j</em></span>.
</p>
<p>
Anyway, we get <span class="math display"><span class="math display">\[\nabla_\theta
f(x,\theta)^T = \begin{bmatrix}   \begin{matrix} x &amp;amp; 0 &amp;amp; \cdots
&amp;amp;  0 \\ 0 &amp;amp; x &amp;amp; \cdots &amp;amp; 0 \\ \vdots  &amp;amp; \vdots &amp;amp;
\ddots &amp;amp; \vdots\\ 0&amp;amp; 0 &amp;amp; \cdots &amp;amp; x\end{matrix} \\
I_{n}\end{bmatrix}, \nabla_\theta f(x,\theta)^T \delta =
\begin{bmatrix}   x \delta_1 \\ x \delta_2\\ \vdots\\ x \delta_n \\
\delta\end{bmatrix}\]</span></span>
</p>
<p>
The update for the vector of parameters <span class="math inline">(<em>W</em><sup>(row&nbsp;1)</sup>)<sup><em>T</em></sup></span> can be written as <span class="math display">(<em>W</em><sup>(row&nbsp;1)</sup>)<sup><em>T</em></sup> ← (<em>W</em><sup>(row&nbsp;1)</sup>)<sup><em>T</em></sup> + <em>δ</em><sub>1</sub><em>x</em>, or</span> <span class="math display"><em>W</em><sup>(row&nbsp;1)</sup> ← <em>W</em><sup>(row&nbsp;1)</sup> + <em>δ</em><sub>1</sub><em>x</em><sup><em>T</em></sup></span> We can stack these updates into a matrix update for <span class="math inline"><em>W</em></span> and vector update for <span class="math inline"><em>b</em></span> that would look like <span class="math display"><em>W</em><sub><em>k</em> + 1</sub> ← <em>W</em><sub><em>k</em></sub> + <em>δ</em><sub><em>k</em></sub>&nbsp;<em>x</em><sup><em>T</em></sup>,</span> <span class="math display"><em>b</em><sub><em>k</em> + 1</sub> ← <em>b</em><sub><em>k</em></sub> + <em>δ</em><sub><em>k</em></sub>,</span> where <span class="math display"><em>δ</em><sub><em>k</em></sub> = <em>y</em> − <em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>),</span> and <span class="math inline"><em>k</em></span> refers to time, not an element of some vector <span class="math inline"><em>δ</em></span>.
</p>
<h2 id="nonlinear-activation" class="anchored">
Nonlinear Activation
</h2>
<p>
We derive the weight updates when considering a single-layer neural network, also known as a <strong>perceptron</strong>. This network is the composition of a linear function with an element-wise application of a nonlinear activation function <span class="math inline"><em>σ</em></span>: <span class="math display"><span class="math display">\[\begin{aligned}
  y(x,\Theta) = \sigma.(f(x,\theta)) = \sigma.(W x + b),
\end{aligned}\]</span></span> where <span class="math inline"><em>σ</em>.(⋅)</span> indicates an element-wise application of a function <span class="math inline"><em>σ</em></span> with scalar inputs to a vector. We may look at the <span class="math inline"><em>i</em><sup>th</sup></span> element of the output <span class="math inline"><em>y</em></span>, and write it as <span class="math display"><em>ŷ</em><sub><em>i</em></sub>(<em>x</em>,<em>θ</em>) = <em>σ</em>(<em>f</em><sub><em>i</em></sub>(<em>x</em>,<em>θ</em>)) = <em>σ</em>.(<em>W</em><sub><em>i</em></sub><em>x</em>+<em>b</em><sub><em>i</em></sub>).</span> From the chain rule, <span class="math display"><span class="math display">\[\frac{\partial
}{\partial \theta} \hat y_i(x,\theta) = \left. \frac{\partial
\sigma}{\partial z} \right|_{ z = f_i(x,\theta) = W_i x + b_i}
\frac{\partial }{\partial \theta}f_i(x,\theta).\]</span></span> Let <span class="math inline"><em>Σ</em>(<em>z</em>)</span> be a diagonal matrix with <span class="math inline"><em>i</em><sup>th</sup></span> entry <span class="math inline"><em>σ</em>(<em>z</em><sub><em>i</em></sub>)</span>. Similarly, let <span class="math inline"><em>Σ</em>′(<em>z</em>)</span> be a diagonal matrix with <span class="math inline"><em>i</em><sup>th</sup></span> entry <span class="math inline"><em>σ</em>′(<em>z</em><sub><em>i</em></sub>)</span>. We collect the <span class="math inline"><em>n</em></span> gradients to get <span class="math display">∇<sub><em>θ</em></sub><em>ŷ</em>(<em>x</em>,<em>θ</em>) = <em>Σ</em>′(<em>z</em>)∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>).</span>
</p>
<p>
We’ve already evaluated the second matrix in the expression above. If we pre-multiply <span class="math inline">∇<sub><em>θ</em></sub><em>f</em>(<em>x</em>,<em>θ</em>)</span> by a matrix <span class="math inline"><em>M</em></span>, then the update for <span class="math inline"><em>W</em></span> and <span class="math inline"><em>b</em></span> would become <span class="math display"><span class="math display">\[\begin{aligned}
W_{k+1} &amp;amp;\gets W_{k} + M^T \delta_k\ x^T,\\
b_{k+1} &amp;amp;\gets b_{k} + M^T \delta_k,
\end{aligned}\]</span></span>
</p>
<p>
Noting that <span class="math inline"><em>Σ</em>′(<em>ŷ</em>) = <em>Σ</em>′(<em>ŷ</em>)<sup><em>T</em></sup></span>, the <span class="math inline"><em>W</em></span> and <span class="math inline"><em>b</em></span> update would simply become <span class="math display"><span class="math display">\[\begin{aligned}
W_{k+1} &amp;amp;\gets W_{k} + \Sigma'(\hat y) \delta_k x^T\\
b_{k+1} &amp;amp;\gets b_{k} + \Sigma'(\hat y) \delta_k,
\end{aligned}\]</span></span> where <span class="math inline"><em>δ</em><sub><em>k</em></sub> = <em>y</em> − <em>σ</em>.(<em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>))</span>. Note that <span class="math inline"><em>σ</em>(⋅)</span> and <span class="math inline"><em>σ</em>′(⋅)</span> are very simple for ReLu functions and sigmoids. Note that if our network consists of a <strong>single</strong> layer, we can update the neurons weights by observing the error at the output, and the input. Each node has access to the input, but we must broadcast the network error to each node. Each node’s update can be implemented ‘row-wise’, making learning local to each node.
</p>
<h2 id="perceptron" class="anchored">
Perceptron
</h2>
<p>
The perceptron update algorithm leaves the term <span class="math inline"><em>Σ</em>′(<em>ŷ</em>)</span> out. This omission is justified for a sigmoid activation functions since <span class="math inline"><em>σ</em>′(<em>a</em>) ≥ 0</span> for any <span class="math inline"><em>x</em></span>, so we are merely rescaling the individual updates. However, we are no longer performing a steepest gradient descent update.
</p>
<h1 id="stacking-layers">
Stacking Layers
</h1>
<h2 id="model-with-single-hidden-layer" class="anchored">
Model With Single Hidden Layer
</h2>
<p>
Now, we add the hidden layer. <span class="math display"><em>ŷ</em> = <em>f</em>(<em>x</em>,<em>Θ</em>) = <em>σ</em>.(<em>W</em><sup><em>o</em></sup>(<em>σ</em>.(<em>W</em><sup><em>i</em></sup><em>x</em>+<em>b</em><sup><em>i</em></sup>))+<em>b</em><sup><em>o</em></sup>)</span> Or simply <span class="math display"><em>ŷ</em>(<em>z</em>,<em>θ</em><sup><em>o</em></sup>) = <em>σ</em>.(<em>W</em><sup><em>o</em></sup><em>z</em>+<em>b</em><sup><em>o</em></sup>),  <em>z</em>(<em>x</em>,<em>θ</em><sup><em>i</em></sup>) = <em>σ</em>.(<em>W</em><sup><em>i</em></sup><em>x</em>+<em>b</em><sup><em>i</em></sup>)</span> The updates for <span class="math inline"><em>W</em><sup><em>o</em></sup></span> and <span class="math inline"><em>b</em><sup><em>o</em></sup></span> don’t change, since <span class="math inline"><em>z</em></span> is as good as a constant input no matter what the values of these parameters are: <span class="math display"><span class="math display">\[\begin{aligned}
W_{k+1}^o &amp;amp;\gets W_{k}^o + \Sigma'(W^o z + b^o) \delta_k^o z^T,\\  
b_{k+1}^o &amp;amp;\gets b_{k}^o + \Sigma'(W^o z + b^o) \delta_k^o.
\end{aligned}\]</span></span>
</p>
<p>
Fortunately, the gradient for <span class="math inline"><em>θ</em><sup><em>i</em></sup></span> doesn’t get too complicated. Notice that <span class="math display">∇<sub><em>θ</em><sup><em>i</em></sup></sub><em>f</em>(<em>x</em>,<em>Θ</em>) = ∇<sub><em>z</em></sub><em>ŷ</em>(<em>z</em>,<em>θ</em>)∇<sub><em>θ</em><sup><em>i</em></sup></sub><em>z</em>(<em>x</em>,<em>θ</em><sup><em>i</em></sup>) = <em>W</em><sup><em>o</em></sup>∇<sub><em>θ</em><sup><em>i</em></sup></sub><em>z</em>(<em>x</em>,<em>θ</em><sup><em>i</em></sup>)</span>
</p>
<p>
We’ve seen this pattern before, and we can therefore claim that the updates for hidden-layer parameters <span class="math inline"><em>W</em><sup><em>i</em></sup>, <em>b</em><sup><em>i</em></sup></span> – with nonlinearity accounted for – will be <span class="math display"><span class="math display">\[\begin{aligned}
  W_{k+1}^i &amp;amp;\gets W_{k}^i + \Sigma'(W^i x + b^i) \left( W^o
\right)^T \Sigma'(W^o z + b^o) \delta_k^o x^T,\\
b_{k+1}^i &amp;amp;\gets b_{k}^i + \Sigma'(W^i x + b^i) \left( W^o
\right)^T  \Sigma'(W^o z + b^o)\delta_k^o.
\end{aligned}\]</span></span>
</p>
<p>
Remember that <span class="math inline"><em>δ</em><sub><em>k</em></sub><sup><em>o</em></sup> = <em>y</em> − <em>Σ</em>(<em>f</em>(<em>x</em>,<em>θ</em><sub><em>k</em></sub>))</span> is the full model’s prediction error using the parameters at iteration <span class="math inline"><em>k</em></span>. Define <span class="math inline"><em>δ</em><sub><em>k</em></sub><sup><em>i</em></sup> = <em>Σ</em>′(<em>W</em><sup><em>i</em></sup><em>x</em>+<em>b</em><sup><em>i</em></sup>)(<em>W</em><sup><em>o</em></sup>)<sup><em>T</em></sup><em>Σ</em>′(<em>W</em><sup><em>o</em></sup><em>z</em>+<em>b</em><sup><em>o</em></sup>)<em>δ</em><sub><em>k</em></sub><sup><em>o</em></sup></span>. Then, the update equations for the hidden layer look like : <span class="math display"><span class="math display">\[\begin{aligned}
  W_{k+1}^i &amp;amp;\gets W_{k}^i +  \delta^i_k x^T,\\
b_{k+1}^i &amp;amp;\gets b_{k}^i + \delta^i_k.
\end{aligned}\]</span></span>
</p>
<p>
These equations resemble the update for a single layer model with no hidden layer. The important idea is that if this hidden layer could observe <span class="math inline"><em>δ</em><sub><em>k</em></sub><sup><em>i</em></sup></span>, it could implement a simple local rule just like the single-layer case. <strong>Error Backpropagation is precisely the task of propagating <span class="math inline"><em>δ</em><sub><em>k</em></sub></span> to <span class="math inline"><em>δ</em><sub><em>k</em></sub><sup><em>i</em></sup></span>. However, the use of <span class="math inline">(<em>W</em><sup><em>o</em></sup>)<sup><em>T</em></sup></span> makes backpropagation non-local to nodes, although it is local to layers.</strong>
</p>
<p>
To avoid using <span class="math inline">(<em>W</em><sup><em>o</em></sup>)<sup><em>T</em></sup></span>, some methods use a feedback layer <span class="math inline"><em>B</em></span> so that <span class="math inline"><em>δ</em><sub><em>k</em></sub><sup><em>i</em></sup> = <em>B</em><em>δ</em><sub><em>k</em></sub></span>. The update rule for <span class="math inline"><em>W</em><sup><em>o</em></sup></span> should hopefully make <span class="math inline"><em>W</em><sup><em>o</em></sup> → <em>B</em><sup><em>T</em></sup></span>, an approach known as feedback weight alignment. Note that with this approach, all learning is performed by choosing the affine terms <span class="math inline"><em>b</em></span> for each node, since <span class="math inline"><em>W</em><sup><em>o</em></sup> → <em>B</em><sup><em>T</em></sup></span>.
</p>
<p>
A more generic approach is to design update rules for <span class="math inline"><em>W</em><sup><em>o</em></sup></span> and <span class="math inline"><em>B</em></span> so that <span class="math inline"><em>W</em><sup><em>o</em></sup> = <em>B</em><sup><em>T</em></sup></span>.
</p>
<h2 id="simpler-single-hidden-layer" class="anchored">
Simpler Single Hidden Layer
</h2>
<p>
Consider the model <span class="math display"><em>ŷ</em> = <em>f</em>(<em>x</em>,<em>Θ</em>) = <em>W</em>(<em>σ</em>.(<em>H</em><em>x</em>+<em>b</em>)) + <em>c</em></span> The updates are then <span class="math display"><span class="math display">\[\begin{aligned}
W_{k+1} &amp;amp;\gets W_{k} + \delta_k^o z^T,\\  
c_{k+1} &amp;amp;\gets c_{k} + \delta_k^o.
\end{aligned}\]</span></span>
</p>
<p>
<span class="math display"><span class="math display">\[\begin{aligned}
  H_{k+1} &amp;amp;\gets H_{k} + \Sigma'(H x + b) W^T  \delta_k^o x^T,\\
b_{k+1} &amp;amp;\gets b_{k} + \Sigma'(H x + b) W^T  \delta_k^o.
\end{aligned}\]</span></span>
</p>
<h2 id="multiple-layers" class="anchored">
Multiple layers
</h2>
<p>
When we have <span class="math inline"><em>L</em></span> layers, the network function may be written as <span class="math display"><span class="math display">\[\begin{aligned}
  y = x_L = \sigma\ \circ \mathrm{Aff}_L \circ \sigma\ \circ
\mathrm{Aff}_{L-1} \circ \sigma\ \circ \mathrm{Aff}_{L-2}  \circ \cdots
\circ \sigma\ \circ \mathrm{Aff}_{2} \circ \sigma\ \circ
\mathrm{Aff}_{1} (x_0).
\end{aligned}\]</span></span> The output of layer <span class="math inline"><em>l</em></span> is <span class="math inline"><em>x</em><sub><em>l</em></sub></span>, which becomes the input to layer <span class="math inline"><em>l</em> + 1</span>. So, the first layer has input <span class="math inline"><em>x</em><sub>0</sub></span>, the network’s input. Note that in some cases the last nonlinearity is actually identity, allowing the output to be an affine function <span class="math inline">Aff<sub><em>L</em></sub></span> of a corresponding <span class="math inline"><em>L</em> − 1</span> layer neural network.
</p>
<p>
The generic update at layer <span class="math inline"><em>l</em></span> is <span class="math display"><span class="math display">\[\begin{aligned}
  W_l &amp;amp;\gets W_l + \eta \delta_l x_{l-1}^T\\
  b_l &amp;amp;\gets b_l + \eta \delta_l, \text{ where}\\
  z_l&amp;amp; = \mathrm{Aff}_l(x_{l-1})
\end{aligned}\]</span></span> <span class="math inline"><em>δ</em><sub><em>l</em></sub>:</span> feedback error at layer <span class="math inline"><em>l</em></span> is given by <span class="math display"><em>δ</em><sub><em>l</em></sub> = <em>Σ</em>′(<em>z</em><sub><em>l</em></sub>)<em>W</em><sub><em>l</em> + 1</sub><sup><em>T</em></sup>⋯<em>Σ</em>′(<em>z</em><sub><em>L</em> − 3</sub>)<em>W</em><sub><em>L</em> − 2</sub><sup><em>T</em></sup><em>Σ</em>′(<em>z</em><sub><em>L</em> − 2</sub>)<em>W</em><sub><em>L</em> − 1</sub><sup><em>T</em></sup><em>Σ</em>′(<em>z</em><sub><em>L</em> − 1</sub>)<em>W</em><sub><em>L</em></sub><sup><em>T</em></sup><em>Σ</em>′(<em>z</em><sub><em>L</em></sub>)<em>e</em>.</span> <span class="math inline"><em>e</em>:</span> error at output layer <span class="math inline"><em>e</em> = <em>y</em> − <em>x</em><sub><em>L</em></sub></span><br> <span class="math inline"><em>L</em>:</span> number of layers<br> <span class="math inline"><em>x</em><sub><em>l</em></sub>:</span> output of layer <span class="math inline"><em>l</em></span><br> <span class="math inline"><em>x</em><sub><em>l</em></sub>:</span> input to layer <span class="math inline"><em>l</em> + 1</span><br> <span class="math inline"><em>η</em>:</span> learning rate<br> For <span class="math inline"><em>n</em> = 1</span>, NO hidden layer, we get <span class="math display"><span class="math display">\[\begin{aligned}
\delta_1 = \Sigma'(z_{1}) e
\end{aligned}\]</span></span>
</p>
<p>
For <span class="math inline"><em>n</em> = 2</span>, a single hidden layer, we get <span class="math display"><span class="math display">\[\begin{aligned}
  \delta_1 &amp;amp;= \Sigma'(z_{1}) W_2^T \Sigma'(z_{2}) e,\\
   \delta_2 &amp;amp;= \Sigma'(z_{2}) e
\end{aligned}\]</span></span>
</p>
<p>
For <span class="math inline"><em>n</em> = 3</span>, two hidden layers, we get <span class="math display"><span class="math display">\[\begin{aligned}
\delta_1 &amp;amp;= \Sigma'(z_{1}) W_2^T \Sigma'(z_{2}) W_3^T \Sigma'(z_{3})
e \\
\delta_2 &amp;amp;= \Sigma'(z_{2}) W_3^T \Sigma'(z_{3}) e\\
\delta_3 &amp;amp;=  \Sigma'(z_{3}) e
\end{aligned}\]</span></span>
</p>
<h1 id="lyapunov-loss">
Lyapunov loss
</h1>
<p>
Consider a model <span class="math inline"><em>V</em> = <em>V</em>(<em>x</em>,<em>θ</em>)</span>. Let <span class="math display"><em>g</em>(<em>x</em>,<em>θ</em>) = <em>V̇</em>(<em>x</em>) + <em>α</em><em>V</em>(<em>x</em>,<em>θ</em>)</span>
</p>
<p>
The loss of the model is <span class="math inline"><em>L</em><sub><em>θ</em></sub>(<em>x</em>) = max (<em>V̇</em>(<em>x</em>)+<em>α</em><em>V</em>(<em>x</em>,<em>θ</em>),0)</span>. Let <span class="math display"><span class="math display">\[\mathrm{posind}(x) = \begin{cases} 1
&amp;amp; \text{if } x \geq 0  \\ 0 &amp;amp;  \text{if } x &amp;lt; 0
\end{cases}\]</span></span> The partial derivative of <span class="math inline"><em>L</em></span> with respect to <span class="math inline"><em>θ</em></span> is <span class="math display"><span class="math display">\[\frac{\partial L}{\partial \theta} =
\mathrm{posind}\left(  g(x,\theta)  \right) \frac{\partial g}{\partial
\theta}\]</span></span>
</p>
<p>
For ReLU NNs, we expect that at a point <span class="math inline"><em>x</em><sub><em>i</em></sub></span>, <span class="math display"><span class="math display">\[\begin{aligned}
  V(x_i) &amp;amp;= p^T x_i + q_i\\
  \dot V(x_i) &amp;amp;= p^T (A_i x_i + a_i)\\
\implies  g(x_i,\theta ) &amp;amp;= p^T (A_i x_i + a_i) + \alpha  p^T x_i +
\alpha q_i\\
&amp;amp;= p^T (A_i + \alpha I) x_i + p^T a_i +\alpha q_i
\end{aligned}\]</span></span> Recall that <span class="math inline"><em>θ</em> = (<em>W</em><sub><em>v</em></sub>,<em>H</em><sub><em>v</em></sub>,<em>b</em><sub><em>v</em></sub>)</span>, so that <span class="math display"><em>V</em>(<em>x</em>,<em>θ</em>) = <em>W</em><sub><em>v</em></sub><em>σ</em><sub>+.</sub>(<em>H</em><sub><em>v</em></sub><em>x</em>+<em>b</em><sub><em>v</em></sub>) = <em>W</em><sub><em>v</em></sub><em>D</em>(<em>x</em>)<em>H</em><sub><em>v</em></sub><em>x</em> + <em>W</em><sub><em>v</em></sub><em>D</em>(<em>x</em>)<em>b</em><sub><em>v</em></sub></span> Therefore, at <span class="math inline"><em>x</em><sub><em>i</em></sub></span>, <span class="math display"><span class="math display">\[\begin{aligned}
  p^T &amp;amp;=  W_v D(x_i) H_v\\
  q &amp;amp;=  W_v D(x_i) b_v
\end{aligned}\]</span></span>
</p>
<p>
Finally, <span class="math display"><span class="math display">\[\begin{aligned}
  g(x_i,\theta ) &amp;amp;= W_v D(x_i) H_v (A_i + \alpha I) x_i + W_v D(x_i)
H_v a_i +\alpha W_v D(x_i) b_v
\end{aligned}\]</span></span> So, the steepest descent direction is <span class="math display"><span class="math display">\[\begin{aligned}
  \Delta W_v &amp;amp;\gets D(x_i) H_v (A_i + \alpha I) x_i +D(x_i) H_v a_i
+ \alpha D(x_i) b_v  \\
  \Delta H_v &amp;amp;\gets D(x_i) W_v^T x_i^T (A_i^T + \alpha I) +  D(x_i)
W_v^T a_i^T\\
  \Delta b_v &amp;amp;\gets \alpha D(x_i) W_v^T
\end{aligned}\]</span></span>
</p>
<p>
In general, for <span class="math inline"><em>ẋ</em> = <em>f</em>(<em>x</em><sub><em>i</em></sub>)</span>, <span class="math display"><span class="math display">\[\begin{aligned}
g(x_i,\theta ) &amp;amp;= p^T \left( f(x_i) + \alpha x_i \right) + \alpha
q_i\\
&amp;amp;= W_v D(x_i) H_v \left( f(x_i) + \alpha x_i \right) + \alpha W_v
D(x_i) b_v
\end{aligned}\]</span></span> <span class="math display">$$
<span class="math display">\[\begin{aligned}
  \Delta W_v &amp;amp;\gets D(x_i) H_v \left( f(x_i)+\alpha x_i \right) +
\alpha D(x_i) b_v \\
  \Delta H_v &amp;amp;\gets D(x_i) W_v^T \left( f(x_i)+\alpha x_i \right)^T
\\
  \Delta b_v &amp;amp;\gets \alpha D(x_i) W_v^T
  
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;h1 id="sec:jacobianconstraints"&gt;Constrained Weights For Gravity
Learning&lt;/h1&gt;
&lt;div class="AsciiList"&gt;
&lt;p&gt;&lt;span&gt;*,-&lt;/span&gt; * Since &lt;span
class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; is the partial
derivative of a potential function, its own partial derivative must be
symmetric. So, if &lt;span
class="math inline"&gt;&lt;em&gt;q&lt;/em&gt; ∈ ℝ&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;, then &lt;span
class="math inline"&gt;$\frac{\partial G_1(q)}{\partial q_2} =
\frac{\partial G_2(q)}{\partial q_1}$&lt;/span&gt; must hold. * This
constraint may be incorporated as a penalty term, or as ML loves to say,
another loss term. * ‘Flux.jl‘ does not want to take the partial
derivative (wrt parameters) of a partial derivative (wrt input) of an
ANN. So we do it manually.&lt;/p&gt;
&lt;/div&gt;
&lt;h2 id="single-hidden-layer"&gt;Single-Hidden Layer&lt;/h2&gt;
&lt;p&gt;Consider a ReLu NN with one hidden layer, and linear outputs &lt;span
class="math inline"&gt;&lt;em&gt;y&lt;/em&gt; = &lt;em&gt;Ĝ&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;q&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sup&gt;) + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
where &lt;span
class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;) = (&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;),&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;))&lt;/span&gt;,
and the dot in &lt;span class="math inline"&gt;&lt;em&gt;σ&lt;/em&gt;.&lt;/span&gt; indicates an
element-wise map of a scalar-domain &lt;span
class="math inline"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; function applied to a vector.&lt;/p&gt;
&lt;p&gt;We want to minimize the additional loss term &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
L_2(q,\theta)&amp;amp;= \left( \frac{\partial \hat G_1(q,\theta)}{\partial
q_2} - \frac{\partial \hat G_2(q,\theta)}{\partial q_1}  \right)^2  \\
&amp;amp;=  \left( e_1^T  \frac{\partial \hat G(q,\theta)}{\partial q} e_2 -
e_2^T  \frac{\partial \hat G(q,\theta)}{\partial q} e_1\right)^2\\
&amp;amp; =  \left(e_1^T J(q,\theta) e_2 - e_2^T J(q,\theta) e_1\right)^2 \\
&amp;amp; = \delta(q,\theta)^2,
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; where &lt;span
class="math inline"&gt;{&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;}&lt;/span&gt; is the
standard basis of &lt;span
class="math inline"&gt;ℝ&lt;sup&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; Let’s name some
intermediate terms: &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
p &amp;amp;= W^i q + b^i &amp;amp;  z &amp;amp;= \sigma.(p)  \\
v &amp;amp;= W^h z + b^h &amp;amp;  y &amp;amp;= \sigma.(v)  \\
y_o &amp;amp;= W^o v + b^o &amp;amp;  &amp;amp; \\
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; and note that &lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;a&lt;/em&gt;)&lt;/span&gt; is a diagonal matrix
whose &lt;span class="math inline"&gt;&lt;em&gt;i&lt;/em&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt; element is
&lt;span
class="math inline"&gt;∂&lt;em&gt;σ&lt;/em&gt;/∂&lt;em&gt;x&lt;/em&gt;(&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;,
where &lt;span class="math inline"&gt;&lt;em&gt;a&lt;/em&gt;&lt;/span&gt; is a vector. The
Jacobian &lt;span
class="math inline"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; of &lt;span
class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; is then
&lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  J(q,\theta) &amp;amp;= \frac{\partial \hat G(q,\theta)}{\partial q}  = W^o
\Sigma'(v) W^h \Sigma'(p) W^i \label{eq:jacobianexpression}\\
  \implies  \delta(q,\theta) &amp;amp;= e_1^T W^o \Sigma'(v) W^h \Sigma'(p)
W^i e_2 - e_2^T W^o \Sigma'(v) W^h \Sigma'(p) W^i e_1
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To optimize &lt;span
class="math inline"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt;
wrt &lt;span class="math inline"&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt;, we need the gradient of
&lt;span class="math inline"&gt;&lt;em&gt;δ&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;)&lt;/span&gt; wrt
&lt;span class="math inline"&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt; at &lt;span
class="math inline"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;. This gradient, when &lt;span
class="math inline"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; is a ReLu function, is (skipping a
factor of 2): &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \frac{\partial \delta(q,\theta)}{\partial W^i} &amp;amp;= \left( W^o
\Sigma'(v) W^h \Sigma'(p)\right)^T \left(e_1 e_2^T - e_2
e_1^T  \right)  \\
  &amp;amp;= \Sigma'(p) (W^h)^T \Sigma'(v) (W^o)^T \left(e_1 e_2^T - e_2
e_1^T  \right)  \\
\frac{\partial \delta(q,\theta)}{\partial W^h} &amp;amp;=  \left( W^o
\Sigma'(v) \right)^T \left(e_1 e_2^T - e_2 e_1^T  \right)
\left(   \Sigma'(p) W^i\right)^T \\
  &amp;amp;= \Sigma'(v) (W^o)^T \left(e_1 e_2^T - e_2 e_1^T  \right)(W^i)^T
\Sigma'(p)   \\
\frac{\partial \delta(q,\theta)}{\partial W^o} &amp;amp;= \left(e_1 e_2^T -
e_2 e_1^T  \right) \left( \Sigma'(v) W^h \Sigma'(p) W^i\right)^T \\
  &amp;amp;= \left(e_1 e_2^T - e_2 e_1^T  \right)(W^i)^T \Sigma'(p) (W^h)^T
\Sigma'(v)  \\
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;h2 id="single-layer-architecture"&gt;Single-Layer Architecture&lt;/h2&gt;
&lt;p&gt;The partial derivative expression in&nbsp;&lt;a href="#eq:jacobianexpression"
data-reference-type="eqref"
data-reference="eq:jacobianexpression"&gt;[eq:jacobianexpression]&lt;/a&gt;
assumes a single hidden layer. When evaluating &lt;span
class="math inline"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; at a points, the
matrix &lt;span class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(⋅)&lt;/span&gt; depends on &lt;span
class="math inline"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;, and has binary-valued diagonal
elements. Due to the nature of &lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(⋅)&lt;/span&gt;, we do not expect the Jacobian
to be symmetric at most points.&lt;/p&gt;
&lt;p&gt;One way to ensure that the Jacobian is symmetric at all points by
construction is to remove the hidden layer: &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
y=\hat G(q,\theta) = W^o \sigma.\left( W^i q + b^i \right)  + b^o  
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; The Jacobian &lt;span
class="math inline"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; of &lt;span
class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt; is given by &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  J(q)&amp;amp;= W^o \Sigma'(z) W^i ,\text{ where}\\
  z &amp;amp;= W^i q + b^i
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; If we set &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
then at every possible input point &lt;span
class="math inline"&gt;&lt;em&gt;q&lt;/em&gt;&lt;/span&gt;, the Jacobian is symmetric!
Instead of directly setting these two matrices to be mutual transposes,
we may define an additional loss functions that either penalizes
asymmetry of the Jacobian, or mutual asymmetry of the weight matrices.
Theses losses and their weight updates are shown in Table&nbsp;&lt;a
href="#tab:lossgradients" data-reference-type="ref"
data-reference="tab:lossgradients"&gt;1&lt;/a&gt;. The derivation of these
expressions are in the subsequent sections.&lt;/p&gt;
&lt;h4 id="relaxed-condition."&gt;Relaxed condition.&lt;/h4&gt;
&lt;p&gt;Instead of &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
we just need to find a vector &lt;span
class="math inline"&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt; such that &lt;span
class="math display"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; = diag(&lt;em&gt;k&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;div id="tab:lossgradients"&gt;
&lt;table&gt;
&lt;caption&gt;Weight matrix updates in single-layer network for different
losses. Note that &lt;span
class="math inline"&gt;&lt;em&gt;z&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class="header"&gt;
&lt;th style="text-align: left;"&gt;Loss&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Function (&lt;span
class="math inline"&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Δ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; =  − (∇&lt;sub&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;))&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/th&gt;
&lt;th style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Δ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;) =  − ∇&lt;sub&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/sub&gt;&lt;em&gt;L&lt;/em&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Training&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span class="math inline"&gt;$\frac{1}{2}
\lVert \delta \rVert^2$&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="even"&gt;
&lt;td style="text-align: left;"&gt;Direct Jacobian&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;$\frac{1}{2}\lVert J - J^T \rVert_F^2$&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;−&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;−&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class="odd"&gt;
&lt;td style="text-align: left;"&gt;Indirect Jacobian&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;$\frac{1}{2}\lVert W^i - \left( W^o
\right)^T\rVert_F^2$&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; − (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;
&lt;td style="text-align: center;"&gt;&lt;span
class="math inline"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; − &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;h3 id="training-loss"&gt;Training Loss&lt;/h3&gt;
&lt;p&gt;Consider the single-layer network with output weights: &lt;span
class="math display"&gt;&lt;em&gt;g&lt;/em&gt; = &lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;,&lt;em&gt;Θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)) + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In an expanded form, we may write &lt;span
class="math display"&gt;&lt;em&gt;g&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;z&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;,  &lt;em&gt;z&lt;/em&gt; = &lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;y&lt;/em&gt;),  &lt;em&gt;y&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The updates for &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span
class="math inline"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; don’t change,
since &lt;span class="math inline"&gt;&lt;em&gt;z&lt;/em&gt;&lt;/span&gt; is as good as a
constant input no matter what the values of these parameters are: &lt;span
class="math display"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;,&lt;/span&gt;
&lt;span
class="math display"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The updates for hidden-layer parameters &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;, &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;
– with nonlinearity accounted for – will be &lt;span
class="math display"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)((&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;)&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;,&lt;/span&gt;
&lt;span
class="math display"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt; + 1&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; ← &lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember that &lt;span
class="math inline"&gt;&lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;g&lt;/em&gt; − &lt;em&gt;Σ&lt;/em&gt;(&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;,&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;))&lt;/span&gt;
is the full model’s prediction error using the parameters at iteration
&lt;span class="math inline"&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So, the updates are &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \underbrace{\Delta W^o}_{\mathbb{R}^{n \times m}} =
\underbrace{\delta_k}_{\mathbb{R}^{n \times 1}}
\underbrace{z^T}_{\mathbb{R}^{1 \times m}}, \quad \underbrace{\Delta
W^i}_{\mathbb{R}^{m \times n}} =\underbrace{\Sigma'(y)}_{\mathbb{R}^{m
\times m}} \underbrace{(W^o)^T}_{\mathbb{R}^{m \times n}}
\underbrace{\delta_k x^T}_{\mathbb{R}^{n \times n}}
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; However, note that &lt;span
class="math inline"&gt;&lt;em&gt;z&lt;/em&gt; = &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;x&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;) ⟹ &lt;em&gt;z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt; = ((&lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;+&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;)&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)&lt;/span&gt;,
so that &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  {\Delta W^o} =  {\delta_k (b^i)^T \Sigma'(y)+ \delta_k x^T(W^i)^T }
\Sigma'(y), \quad {\Delta W^i} ={\Sigma'(y)}{(W^o)^T}{\delta_k x^T}
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; and &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \left( \Delta W^o \right)^T =  \Sigma'(y) b^i \delta_k^T + \Sigma'(y)
W^i \left( \delta_k x^T \right)^T  , \quad {\Delta W^i}
={\Sigma'(y)}{(W^o)^T}{\delta_k x^T}
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; We may take the difference to obtain &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \left( \Delta W^o \right)^T - \Delta W^i = \Sigma'(y) b^i \delta_k^T +
\Sigma'(y) \left(   W^i  Z^T - (W^o)^T  Z   \right),
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; where &lt;span
class="math inline"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;δ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;k&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.
Let &lt;span
class="math inline"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt; + &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.
Then, &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \left( \Delta W^o \right)^T - \Delta W^i = \Sigma'(y) b^i \delta_k^T +
\Sigma'(y) \left(   W^i  - (W^o)^T     \right) Z^{sym} + \Sigma'(y)
\left(   W^i  + (W^o)^T     \right) Z^{asym},
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;When &lt;span
class="math inline"&gt;&lt;em&gt;b&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = 0&lt;/span&gt; and &lt;span
class="math inline"&gt;&lt;em&gt;Z&lt;/em&gt; = &lt;em&gt;Z&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
the loss gradient would cause updates in a way that reduces the
difference between &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt; and &lt;span
class="math inline"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
which would make the Jacobian symmetric.&lt;/p&gt;
&lt;p&gt;We may derive this expression in another way. Note that &lt;span
class="math inline"&gt;&lt;em&gt;σ&lt;/em&gt;.(&lt;em&gt;z&lt;/em&gt;) = &lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;z&lt;/em&gt;)&lt;em&gt;z&lt;/em&gt;&lt;/span&gt;.
Therefore, &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
g &amp;amp;= f(x,\Theta) = W^o (\sigma.(W^i x + b^i)   ) + b^o \\
&amp;amp;= W^o \Sigma'(z) W^i x + W^o \Sigma'(z) b^i + b^o
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Define &lt;span class="math inline"&gt;$L = \frac{1}{2}\lVert
\delta\rVert^2 =  \sum_j \frac{1}{2} \delta_j^2$&lt;/span&gt;. The gradient
w.r.t. &lt;span class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;
is &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
\frac{\partial L}{\partial W^o} &amp;amp;= \frac{\partial L}{\partial
\delta}\frac{\partial \delta}{\partial W^o} =   \sum_j
\delta_j  \frac{\partial \delta_j}{\partial W^o}=   \sum_j
\delta_j  \frac{\partial (-f_j(x,\theta))}{\partial W^o}\\
&amp;amp;=- \sum_j \delta_j \frac{\partial \left( e_j^T   W^o \Sigma'(z) W^i
x + e_j^T  W^o \Sigma'(z) b^i  \right)}{\partial W^o}\\
&amp;amp;=  - \sum_j \delta_j e_j \left( x^T \left( W^i \right)^T
\Sigma'(z)  \right)- \sum_j \delta_j e_j \left( \left( b^i \right)^T
\Sigma'(z) \right) \\
&amp;amp;= - \delta x^T \left( W^i \right)^T \Sigma'(z) - \delta \left( b^i
\right)^T \Sigma'(z)
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; Similarly, we may calculate &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \frac{\partial L}{\partial W^i} &amp;amp;= -\sum_j \delta_j   \left(
\Sigma'(z)  \left( W^o \right)^T e_j x^T  \right) \\
  &amp;amp;= -\Sigma'(z)  \left( W^o \right)^T \delta x^T
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;h3 id="direct-jacobian-loss"&gt;Direct Jacobian Loss&lt;/h3&gt;
&lt;p&gt;If &lt;span
class="math inline"&gt;&lt;em&gt;J&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;,&lt;em&gt;Θ&lt;/em&gt;) = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;Σ&lt;/em&gt;′(&lt;em&gt;y&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
then the Jacobian loss &lt;span
class="math inline"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;J&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;
is given by &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
L_J(\Theta) &amp;amp;= e_1^T W^o \Sigma'(y) W^i e_2 - e_2^T W^o \Sigma'(y)
W^i e_1,
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; where &lt;span
class="math inline"&gt;&lt;em&gt;y&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;q&lt;/em&gt; + &lt;em&gt;b&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.
Similar to the hidden layer case, we derive the gradient of this
function with respect to the network weights. &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \frac{\partial L_J(\Theta)}{\partial W^o} &amp;amp;= e_1 e_2^T \left( W^i
\right)^T \Sigma'(y) - e_2 e_1^T \left( W^i \right)^T\Sigma'(y) = \left(
e_1 e_2^T - e_2 e_1^T \right) \left( W^i \right)^T\Sigma'(y)\\
    \frac{\partial L_J(\Theta)}{\partial W^i} &amp;amp;= \Sigma'(y)  \left(
W^o \right)^T e_1 e_2^T - \Sigma'(y)  \left( W^o \right)^T e_2 e_1^T =
\Sigma'(y)  \left( W^o \right)^T \left( e_1 e_2^T - e_2 e_1^T \right)
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; Together, we get that &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \frac{\partial L_J(\Theta)}{\partial W^o} ^T &amp;amp;= - \Sigma'(y) W^i
\left( e_1 e_2^T - e_2 e_1^T \right) \\
  \frac{\partial L_J(\Theta)}{\partial W^i} &amp;amp; = \Sigma'(y) \left(
W^o \right)^T \left( e_1 e_2^T - e_2 e_1^T \right)
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We may write &lt;span
class="math inline"&gt;&lt;em&gt;L&lt;/em&gt;&lt;sub&gt;&lt;em&gt;J&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;Θ&lt;/em&gt;)&lt;/span&gt;
as &lt;span class="math display"&gt;\]</span>L_J() = <em>{j=1}^m ( w^o</em>{1j} ‘(z_j)w^i_{j2} - w^o_{2j} ’(z_j)w^i_{j1} ).<span class="math display">\[&lt;/span&gt; So, for
symmetry, we don’t need &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; = (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;.
Instead, we need that on average, &lt;span
class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;(:,&lt;em&gt;j&lt;/em&gt;)&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;j&lt;/em&gt;,:)&lt;/span&gt;
is symmetric.&lt;/p&gt;
&lt;h3 id="indirect-jacobian-loss"&gt;Indirect Jacobian Loss&lt;/h3&gt;
&lt;p&gt;This section derives the gradient of the indirect loss on the weights
of the neural network. By indirect, we mean that the goal is to obtain a
symmetric Jacobian, and we define a loss function directly on the
weights that indirectly achieves a symmetric Jacobian. It only works for
a single layer network. The loss is given by &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  L(J(q,\Theta)) = L(W^o,W^i)  = \lVert W^i - \left( W^o
\right)^T\rVert_F^2
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Frobenius norm of a (real-valued) matrix &lt;span
class="math inline"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/span&gt; is &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \lVert M  \rVert_F = \sqrt{ \mathrm{tr\ }M M^T}
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; As shown in &lt;a
href="https://web.stanford.edu/~jduchi/projects/matrix_prop.pdf"&gt;these
notes&lt;/a&gt;, &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \nabla_A \mathrm{tr\ } A B &amp;amp;= B^T \\
  \nabla_A \mathrm{tr\ } A B A^T C &amp;amp;= C A B + C^T A B^T
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span
class="math inline"&gt;&lt;em&gt;M&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt; − (&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;,
then &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
\mathrm{tr\ }M M^T  &amp;amp;= \mathrm{tr\ }\left( W^i - \left( W^o
\right)^T \right) \left( \left( W^i \right)^T -  W^o  \right) \\
&amp;amp;= \mathrm{tr\ }W^i\left( W^i \right)^T  - \mathrm{tr\ }W^i W^o  -
\mathrm{tr\ }\left( W^o \right)^T  \left( W^i \right)^T +\mathrm{tr\
}\left( W^o \right)^T  W^o\\
&amp;amp;= \mathrm{tr\ }W^i\left( W^i \right)^T + \mathrm{tr\ }W^o\left( W^o
\right)^T - 2 \mathrm{tr\ }W^i W^o
\end{aligned}\]</span>
W^i W^o \end{aligned}<span class="math display">\[&lt;/span&gt; We have used the following facts: the trace is
linear, the trace is invariant under transposition (adjoint map), and
the trace is invariant under cyclic permutation of matrix products.&lt;/p&gt;
&lt;p&gt;With these details in place, we derive &lt;span
class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \nabla_{W^o} L(W^o,W^i) &amp;amp;=   \nabla_{W^o} \mathrm{tr\ }  \left(
W^i - \left( W^o \right)^T \right) \left( \left( W^i \right)^T
-  W^o  \right) \\
  &amp;amp;= \nabla_{W^o} \left( \mathrm{tr\ }W^i\left( W^i \right)^T +
\mathrm{tr\ }W^o\left( W^o \right)^T - 2 \mathrm{tr\ }W^i W^o \right)\\
  &amp;amp;=2 \left( W^o- \left( W^i \right)^T \right), \text{ and}\\
\nabla_{W^i} L(W^o,W^i) &amp;amp;=  2\left( W^i- \left( W^o \right)^T
\right)
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; The negative gradient of this loss function
brings &lt;span class="math inline"&gt;&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;
and &lt;span
class="math inline"&gt;(&lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sup&gt;)&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;/span&gt;
closer to together.&lt;/p&gt;
&lt;h2 id="ideas-from-lyapunov-work"&gt;Ideas from Lyapunov Work&lt;/h2&gt;
&lt;p&gt;We have been viewing a single-layer ReLU Network as the piecewise
affine map &lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  \Sigma'(z) H x +  \Sigma'(z) b  \geq 0 \implies f(x) = W (\Sigma'(z) H
x +  \Sigma'(z) b)
\end{aligned}\]</span>
<span class="math display">\[&lt;/span&gt; We wanted &lt;span
class="math inline"&gt;&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;) ≤ 0&lt;/span&gt; when &lt;span
class="math inline"&gt;&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;)&lt;/span&gt; was a scalar, and we
used theorems of alternatives to eliminate &lt;span
class="math inline"&gt;&lt;em&gt;x&lt;/em&gt;&lt;/span&gt;. The idea we are exploring is that
on each cell, we need to solve a linear regression problem. This may run
into the whole reason for networks+SGD: you can’t handle millions of
data.&lt;/p&gt;
&lt;h1 id="hessian-descent-for-gravity-learning"&gt;Hessian Descent For
Gravity Learning&lt;/h1&gt;
&lt;p&gt;In Section&nbsp;&lt;a href="#sec:jacobianconstraints"
data-reference-type="ref"
data-reference="sec:jacobianconstraints"&gt;5&lt;/a&gt;, the neural network
learns &lt;span class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;, and we
attempt to constrain the Jacobian of this map to bias the learning. This
biasing does not appear work. One reason may be Stefano Soatto’s &lt;a
href="https://sites.google.com/view/control-meets-learning/home/past-seminars"&gt;talk&lt;/a&gt;
about how there appears to be an intermediate memorization phase before
a generalization phase, corresponding to escaping a rough loss landscape
before arriving at a relatively flat basin. Applying that idea here, we
need to memorize &lt;span class="math inline"&gt;&lt;em&gt;G&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;
without worrying about physics, before applying physics-based
constraints such as symmetry of the Jacobian while training. Note that
in &lt;span&gt;&lt;code&gt;Julia&lt;/code&gt;&lt;/span&gt; experiments on the
&lt;code&gt;RigidBodyDynamics&lt;/code&gt; acrobot model, the original loss has to
become &lt;span class="math inline"&gt;𝒪(10&lt;sup&gt;−4&lt;/sup&gt;)&lt;/span&gt; before seeing
symmetry, a loss of &lt;span class="math inline"&gt;𝒪(10&lt;sup&gt;−2&lt;/sup&gt;)&lt;/span&gt;
is not enough. This observation suggests a hypothesis that
symmetrization occurs in later stages of training.&lt;/p&gt;
&lt;p&gt;We now investigate an alternate strategy, in which the network is
meant to predict the potential energy &lt;span
class="math inline"&gt;&lt;em&gt;P&lt;/em&gt;(&lt;em&gt;q&lt;/em&gt;)&lt;/span&gt;, where &lt;span
class="math inline"&gt;$\frac{\partial P}{\partial q} = G(q)$&lt;/span&gt;. We
now know that any ReLU network has a partial derivative of the form
&lt;span class="math display"&gt;\]</span>
<span class="math display">\[\begin{aligned}
  J(q) = W_L \Sigma_{L-1}' \left( z_{L-1} \right) W_{L-1}\Sigma_{L-2}'
\left( z_{L-2} \right) W_{L-2} \cdots W_1
\end{aligned}\]</span>
$$</span>
</p>
<p>
If our network predicts <span class="math inline"><em>P</em>(<em>q</em>)</span>, then we have a piecewise linear map <span class="math inline"><em>G</em>(<em>q</em>)</span>, with discontinuities at the pieces. In other words, this is <span>a bad idea when using ReLU</span>.
</p>
<h1 id="feedback-alignment">
Feedback Alignment
</h1>
<p>
With the above background, we may begin to investigate network training approaches based on Feedback Alignment. The idea is that back propagation is not biologically plausible because of the reliance on non-local information. Feedback Alignment is a local update rule based on the idea that a network can learn – using local rules – to make the forward pass mimic a static backprop error term, thereby achieving a more biologically plausible backpropogation step.
</p>
<h2 id="what-does-dfa-optimize" class="anchored">
What does DFA optimize?
</h2>
<p>
The desire to achieve alignment is so that the updates optimize <span class="math inline">∥<em>e</em>∥<sup>2</sup>/2</span>. Two questions remain unanswered: 1. using the normal updates, why does <span class="math inline"><em>W</em> → <em>B</em><sup><em>T</em></sup></span> ? Or is the claim untrue outside specific conditions? Presentation you found suggests that there’s no proof backing the empirical performance on deep networks. Found cases where loss is not getting worse as time goes on. 2. Is there a function for which the DFA updates <em>is</em> the currect update?
</p>
<h2 id="learning-to-learn-with-feedback-and-local-plasticity" class="anchored">
Learning to Learn with Feedback and Local Plasticity
</h2>
<ul>
<li>
<p>
The idea of this paper is to use meta-learning to choose the initial weights of a network, and the feedback matrices. %
</p>
</li>
<li>
<p>
Then, learning is through hebbian synaptic pasticity using Oja’s learning rule:<span class="math display"><em>w</em> ← <em>w</em> + <em>α</em>(<em>a</em><em>b</em>−<em>b</em><sup>2</sup><em>w</em>),</span> where the presynaptic activity is <span class="math inline"><em>a</em></span>, and the postsynaptic activity resulting from feedback is <span class="math inline"><em>b</em></span>
</p>
<ul>
<li>
<p>
Oja’s learning rule is an approximation to normalized Hebbian learning, and he show’s that if <span class="math inline"><em>α</em></span> varies in a specific way (eg. <span class="math inline">1/<em>t</em></span>), the network weights will align with the principal component of the input signal. This proof uses some stochastic approximation theory that leads to an ODE.
</p>
</li>
</ul>
</li>
<li>
<p>
Feedback is through a linear map but is NOT of the form <span class="math inline"><em>δ</em><sub><em>i</em></sub> = <em>B</em><sub><em>i</em></sub>(<em>y</em>−<em>ŷ</em>)</span>. For some reason, modified activations are used in the updates: <span class="math display"><em>x</em><sub><em>i</em></sub> ← (1−<em>β</em><sub><em>i</em></sub>)<em>x</em><sub><em>i</em></sub> + <em>β</em><sub><em>i</em></sub><em>R</em><em>e</em><em>L</em><em>U</em>(<em>B</em><sub><em>i</em></sub><em>y</em>−<em>b</em>)</span>
</p>
</li>
</ul>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>